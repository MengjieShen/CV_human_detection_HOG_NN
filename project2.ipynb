{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "project1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "source": [
        "## **Import library**"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "import math\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHQEiOxJaR30"
      },
      "source": [
        "## **Define function for normalization and image display**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TS-KYFMPobPO"
      },
      "source": [
        "#image normalization\n",
        "def normalization(img, range):\n",
        "  normed_img = img/(img.max()/range)\n",
        "  return normed_img"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7mB05cIaZlY"
      },
      "source": [
        "def plotImage(image, title):\n",
        "  plt.imshow(image, 'gray', vmin = 0, vmax = 255)\n",
        "  plt.title(title)\n",
        "  fig = plt.gcf()\n",
        "  fig.set_size_inches(13,13)\n",
        "  plt.show()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCttg_4FcDRC"
      },
      "source": [
        "def convolve2d(image, kernel, stride = 1):\n",
        "  kernel = np.flipud(np.fliplr(kernel))\n",
        "\n",
        "  k_sizeX, k_sizeY = kernel.shape\n",
        "\n",
        "  im_sizeX, im_sizeY = image.shape\n",
        "\n",
        "  padding = int(np.floor((k_sizeX-1)/2)) # padding = ((k-1) / 2)\n",
        "\n",
        "  #output image (convolved with image)\n",
        "  new_image = np.zeros((im_sizeX + 2*padding, im_sizeY + 2*padding))\n",
        "  new_image[padding: im_sizeX+padding, padding: im_sizeY + padding] = image[:,:]\n",
        "\n",
        "  output = np.zeros(new_image.shape)\n",
        "\n",
        "  new_im_sizeX, new_im_sizeY = new_image.shape\n",
        "  for y in range(new_im_sizeY):\n",
        "    if y > new_im_sizeY-k_sizeY:\n",
        "      break\n",
        "\n",
        "    for x in range(new_im_sizeX):\n",
        "      if x > new_im_sizeX-k_sizeX:\n",
        "        break\n",
        "      \n",
        "      if( y % stride == 0 and x%stride == 0):\n",
        "        \n",
        "        output[int(np.floor((2*x+k_sizeX)/2)),int(np.floor((2*y+k_sizeY)/2))] = (kernel * new_image[x:x+k_sizeX, y:y+k_sizeY]).sum()\n",
        "\n",
        "  return output"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def grey_scale(img):\n",
        "    R, G, B = img[:,:,0], img[:,:,1], img[:,:,2]\n",
        "    imgGray = 0.2989 * R + 0.5870 * G + 0.1140 * B\n",
        "    return imgGray"
      ]
    },
    {
      "source": [
        "## **Define Prewitt Operator**"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "Px = np.array([[1, 0, -1],\n",
        "               [1, 0, -1],\n",
        "               [1, 0, -1]])\n",
        "\n",
        "Py = np.array([[1, 1, 1],\n",
        "               [0, 0, 0],\n",
        "               [-1, -1, -1]])"
      ]
    },
    {
      "source": [
        "## **HOG Feature **"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_bin(cell_theta, cell_M):\n",
        "    #calculate the histogram of the each cell\n",
        "    bin_size = 9\n",
        "    bin_degree = 180/bin_size\n",
        "    hist = np.zeros(9)\n",
        "    for i in range(cell_theta.shape[0]):\n",
        "        for j in range(cell_theta.shape[1]):\n",
        "            bin_index = int((cell_theta[i, j] + 10) // 20)\n",
        "     \n",
        "            v_1 = cell_M[i, j] * (bin_degree * (bin_index + 1) - 10 - cell_theta[i,j])/bin_degree\n",
        "            v_2 = cell_M[i, j] * (cell_theta[i,j] - bin_degree * bin_index + 10)/bin_degree\n",
        "            \n",
        "            hist[bin_index] += v_1\n",
        "            if bin_index +1 <= 8:\n",
        "                hist[bin_index+1] += v_2\n",
        "    return hist\n",
        "\n",
        "def block_normalization(block_vec):\n",
        "    temp = np.sqrt(np.sum(np.power((block_vec),2)))\n",
        "    if temp == 0:\n",
        "        return block_vec\n",
        "    return block_vec/temp\n",
        "\n",
        "\n",
        "def get_hist_cell(theta, M):\n",
        "    #get the histogram of the each cell\n",
        "    width, height = M.shape[0], M.shape[1]\n",
        "    bin_size = 9\n",
        "    cell_size = 8\n",
        "    step = 8\n",
        "    hist_vector = np.zeros((int(width/cell_size), int(height/cell_size), bin_size))\n",
        "    for i in range(hist_vector.shape[0]):\n",
        "        for j in range(hist_vector.shape[1]):\n",
        "            cell_magnitude = M[i * cell_size: (i+1) * cell_size, j*cell_size : (j+1) *cell_size]\n",
        "            cell_theta = theta[i * cell_size: (i+1) * cell_size, j*cell_size : (j+1) *cell_size]\n",
        "            cell_hist = get_bin(cell_theta, cell_magnitude)\n",
        "            hist_vector[i][j] = cell_hist\n",
        "            \n",
        "\n",
        "\n",
        "# get the histogram of the whole image\n",
        "    hog_vector = []\n",
        "    for i in range(int(width/cell_size) - 1):\n",
        "        for j in range(int(height/cell_size) -1):\n",
        "            block_vec = []\n",
        "            block_vec.extend(hist_vector[i][j])\n",
        "            block_vec.extend(hist_vector[i][j+1])\n",
        "            block_vec.extend(hist_vector[i+1][j])\n",
        "            block_vec.extend(hist_vector[i+1][j+1])\n",
        "            hog_vector.extend(block_normalization(block_vec))\n",
        "    return hog_vector"
      ]
    },
    {
      "source": [
        "## **Feed into the Neuro Network**"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "class nn():\n",
        "    def __init__(self, X, layer_size, test_X):\n",
        "        np.random.seed(1)\n",
        "        self.X = X.T\n",
        "        self.test_X = test_X.T\n",
        "        # self.y = y\n",
        "        self.ground_truth = np.array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0 ,0 ,0 ,0 ,0]])\n",
        "        self.num = 0\n",
        "        self.test_ground_truth = np.array([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0]])\n",
        "        self.y_pred = np.zeros((self.ground_truth.shape[1],1))\n",
        "        self.sam = self.ground_truth.shape[1]\n",
        "        self.learning_rate = 0.00003\n",
        "        # self.layer_num = 2\n",
        "        self.layer_size = layer_size\n",
        "        self.weights_1 = 2 * np.random.rand(self.layer_size, X.shape[1]) - 1\n",
        "        self.weights_2 = 2 * np.random.rand(1, self.layer_size) - 1\n",
        "        self.bias_1 = np.random.rand(self.layer_size,1)\n",
        "        self.bias_2 = np.random.rand(1,1)\n",
        "        self.test_loss = 0\n",
        "\n",
        "        self.loss = 0\n",
        "        # self.layer_output = feed_forward(X, weights)\n",
        "\n",
        "    def dSigmoid(self,x):\n",
        "        s = 1/(1+np.exp(-x))\n",
        "        return np.multiply(s,(1-s))\n",
        "\n",
        "    def dRelu(self,x):\n",
        "        x[ x <= 0] = 0\n",
        "        x[x > 0] = 1\n",
        "        return x\n",
        "    \n",
        "    def sigmoid_function(self,x):\n",
        "        return 1/(1+np.exp(-x))\n",
        "\n",
        "    def relu_function(self,x):\n",
        "        return np.maximum(0,x)\n",
        "\n",
        "    def loss_function(self, y_true, y_pred):\n",
        "        y_pred[y_pred == 1] = 1 - (1e-8)\n",
        "        loss = (1./self.sam) * (-np.dot(y_true,np.log(y_pred).T) - np.dot(1-y_true, np.log(1-y_pred).T))    \n",
        "        return loss\n",
        "\n",
        "    def train(self):\n",
        "        test_brek = False\n",
        "        for iteration in range(10000):\n",
        "            #feed forward\n",
        "            if iteration % 10 == 0:\n",
        "                test_loss = self.loss_function(self.test_ground_truth, self.pred(self.test_X))\n",
        "                # if the loss for test increase, we stop the iteration\n",
        "                # print(self.y_pred)\n",
        "                if test_loss > self.test_loss:\n",
        "                    self.num += 1\n",
        "                else:\n",
        "                    self.num = 0\n",
        "                if self.num == 3:\n",
        "                    break\n",
        "                self.test_loss = test_loss\n",
        "                \n",
        "                print(\"iteration\" + str(iteration) + \":\" + str(self.y_pred) + \"Loss\" + str(self.loss) + \"test loss:\" + str(self.test_loss))\n",
        "            before_1 = np.dot(self.weights_1, self.X) + self.bias_1\n",
        "            layer1_output = self.relu_function(before_1)\n",
        "            before_2 = np.dot(self.weights_2, layer1_output) + self.bias_2\n",
        "            layer2_output = self.sigmoid_function(before_2)\n",
        "            layer2_output[layer2_output == 1] = 1 - (1e-8)\n",
        "            self.y_pred = layer2_output\n",
        "            self.loss = self.loss_function(self.ground_truth, self.y_pred)\n",
        "\n",
        "\n",
        "            d_y_pred = - np.divide(self.ground_truth, self.y_pred) + np.divide(1-self.ground_truth, 1-self.y_pred)\n",
        "            d_before_2 = np.multiply(d_y_pred,self.dSigmoid(before_2))\n",
        "            d_layer1_output = np.dot(self.weights_2.T, d_before_2)\n",
        "            d_weight_2 = 1./layer1_output.shape[1] * np.dot(d_before_2, layer1_output.T)\n",
        "            d_bais_2 = 1./layer1_output.shape[1] * np.dot(d_before_2, np.ones([d_before_2.shape[1], 1]))\n",
        "            d_before_1 = np.multiply(d_layer1_output,self.dRelu(before_1))\n",
        "            d_input = np.dot(self.weights_1.T, d_before_1)\n",
        "            d_weight_1 = 1./self.X.shape[1] * np.dot(d_before_1, self.X.T)\n",
        "            d_bais_1 = 1./self.X.shape[1] * np.dot(d_before_1, np.ones([d_before_1.shape[1],1]))\n",
        "            self.weights_1 = self.weights_1 - self.learning_rate * d_weight_1\n",
        "            self.bias_1 = self.bias_1 - self.learning_rate * d_bais_1\n",
        "            self.weights_2 = self.weights_2 - self.learning_rate * d_weight_2\n",
        "            self.bias_2 = self.bias_2 - self.learning_rate * d_bais_2\n",
        "            \n",
        "            \n",
        "\n",
        "    def pred(self, input):\n",
        "            before_1 = np.dot(self.weights_1, input) + self.bias_1\n",
        "            layer1_output = self.relu_function(before_1)\n",
        "            before_2 = np.dot(self.weights_2, layer1_output) + self.bias_2\n",
        "            layer2_output = self.sigmoid_function(before_2)\n",
        "            return layer2_output"
      ]
    },
    {
      "source": [
        "## **Prepare the training image**"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_image(img_name):\n",
        "    img = cv2.imread(img_name) \n",
        "    img = grey_scale(img)\n",
        "    Gx = convolve2d(img,Px)\n",
        "    Gy = convolve2d(img,Py)\n",
        "    M = np.sqrt(Gx*Gx + Gy*Gy)\n",
        "    M = np.round(normalization(M, 255))\n",
        "    theta = np.zeros(Gx.shape)\n",
        "    for i in range(theta.shape[0]):\n",
        "        for j in range(theta.shape[1]):\n",
        "            if Gy[i,j] == 0 and Gx[i,j] == 0:\n",
        "                theta[i,j] = 0\n",
        "            elif Gx[i,j] == 0:\n",
        "                theta[i,j] = 90\n",
        "            else:\n",
        "                theta[i,j] = np.arctan2(Gy[i,j], Gx[i,j]) * 180 / np.pi\n",
        "                if theta[i,j] < -10:\n",
        "                    theta[i,j] += 180\n",
        "                elif theta [i,j] >= 170:\n",
        "                    theta[i,j] -= 180\n",
        "    for i in range(theta.shape[0]):\n",
        "        for j in range(theta.shape[1]):\n",
        "            if str(theta[i,j]) == \"nan\":\n",
        "                print(true)\n",
        "    hog_vector= get_hist_cell(theta, M)\n",
        "    return hog_vector"
      ]
    },
    {
      "source": [
        "## **Read in the training data, First positive, then negative**"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "img done!\n",
            "img done!\n",
            "img done!\n",
            "img done!\n",
            "img done!\n",
            "img done!\n",
            "img done!\n",
            "img done!\n",
            "img done!\n",
            "img done!\n",
            "done\n"
          ]
        }
      ],
      "source": [
        "# from skimage import io \n",
        "import os\n",
        "data_input = [] \n",
        "path = \"/Users/shenmengjie/Desktop/Computer Vision/project2/data/Train_Positive/\"\n",
        "path_2 = \"/Users/shenmengjie/Desktop/Computer Vision/project2/data/Train_Negative/\"\n",
        "\n",
        "file_dir = os.listdir(path)\n",
        "for file in file_dir:\n",
        "    if not os.path.isdir(file):\n",
        "        file_name = path + file\n",
        "        # img = cv2.imread(file_name)\n",
        "        # plotImage(img, \"test\")\n",
        "        # print(file_name)\n",
        "        hog_vector = prepare_image(file_name)\n",
        "        data_input.append(hog_vector)\n",
        "        print(\"img done!\")\n",
        "    else:\n",
        "        print(\"cannot open the file!\")\n",
        "\n",
        "# print(\"positive done!\")\n",
        "file_dir_2 = os.listdir(path_2)\n",
        "for file in file_dir_2:\n",
        "    if not os.path.isdir(file):\n",
        "        file_name = path_2 + file\n",
        "        hog_vector = prepare_image(file_name)\n",
        "        data_input.append(hog_vector)\n",
        "    else:\n",
        "        print(\"cannot open the file!\")\n",
        "data_input = np.matrix(data_input)\n",
        "print(\"done\")"
      ]
    },
    {
      "source": [
        "## **Read in the test data**"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "test_input = [] \n",
        "path = \"/Users/shenmengjie/Desktop/Computer Vision/project2/data/Test_Positive/\"\n",
        "path_2 = \"/Users/shenmengjie/Desktop/Computer Vision/project2/data/Test_Negative/\"\n",
        "\n",
        "file_dir = os.listdir(path)\n",
        "for file in file_dir:\n",
        "    if not os.path.isdir(file):\n",
        "        file_name = path + file\n",
        "        # img = cv2.imread(file_name)\n",
        "        # plotImage(img, \"test\")\n",
        "        # print(file_name)\n",
        "        hog_vector = prepare_image(file_name)\n",
        "        test_input.append(hog_vector)\n",
        "    else:\n",
        "        print(\"cannot open the file!\")\n",
        "\n",
        "# print(\"positive done!\")\n",
        "file_dir_2 = os.listdir(path_2)\n",
        "for file in file_dir_2:\n",
        "    if not os.path.isdir(file):\n",
        "        file_name = path_2 + file\n",
        "        # print(file_name)\n",
        "        # img = cv2.imread(file_name)\n",
        "        # plotImage(img, \"test\")\n",
        "        # print(file_name)\n",
        "        hog_vector = prepare_image(file_name)\n",
        "        test_input.append(hog_vector)\n",
        "    else:\n",
        "        print(\"cannot open the file!\")\n",
        "test_input = np.matrix(test_input)\n",
        "print(\"done\")"
      ]
    },
    {
      "source": [
        "## **Train a neural network with layer size of 250 neurons**"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "tags": [
          "outputPrepend"
        ]
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "e-02 4.64701390e-27 5.47009709e-09\n",
            "  3.36926875e-38 4.69615752e-34 2.83720352e-01 2.55776324e-01]]Loss[[0.22942169]]test loss:[[4.62003373]]\n",
            "iteration3840:[[9.99996514e-01 1.00000000e+00 1.00000000e+00 9.99999990e-01\n",
            "  9.99999990e-01 9.99409380e-01 9.98290872e-01 3.23901650e-02\n",
            "  9.99999997e-01 9.99995268e-01 1.50928894e-13 1.73160495e-15\n",
            "  2.94447827e-01 7.28142574e-02 4.76652465e-27 5.58483262e-09\n",
            "  3.44802095e-38 4.80188811e-34 2.82418233e-01 2.55476621e-01]]Loss[[0.22417327]]test loss:[[4.61503073]]\n",
            "iteration3850:[[9.99996629e-01 1.00000000e+00 1.00000000e+00 9.99999990e-01\n",
            "  9.99999990e-01 9.99426454e-01 9.98344733e-01 3.58527129e-02\n",
            "  9.99999997e-01 9.99995408e-01 1.54594465e-13 1.78358253e-15\n",
            "  2.94078442e-01 7.30110105e-02 4.88790010e-27 5.70044656e-09\n",
            "  3.52777036e-38 4.90879150e-34 2.81090971e-01 2.55133239e-01]]Loss[[0.21896047]]test loss:[[4.61009612]]\n",
            "iteration3860:[[9.99996739e-01 1.00000000e+00 1.00000000e+00 9.99999990e-01\n",
            "  9.99999990e-01 9.99442810e-01 9.98396232e-01 3.96441337e-02\n",
            "  9.99999997e-01 9.99995542e-01 1.58303364e-13 1.83658229e-15\n",
            "  2.93626872e-01 7.31833855e-02 5.01038971e-27 5.81675457e-09\n",
            "  3.60797440e-38 5.01616369e-34 2.79734152e-01 2.54719236e-01]]Loss[[0.21378614]]test loss:[[4.60522736]]\n",
            "iteration3870:[[9.99996844e-01 1.00000000e+00 1.00000000e+00 9.99999990e-01\n",
            "  9.99999990e-01 9.99458502e-01 9.98445530e-01 4.37866823e-02\n",
            "  9.99999997e-01 9.99995670e-01 1.62050596e-13 1.89055085e-15\n",
            "  2.93101282e-01 7.33336512e-02 5.13408018e-27 5.93355021e-09\n",
            "  3.68870062e-38 5.12406857e-34 2.78343104e-01 2.54242154e-01]]Loss[[0.20865599]]test loss:[[4.60042261]]\n",
            "iteration3880:[[9.99996945e-01 1.00000000e+00 1.00000000e+00 9.99999990e-01\n",
            "  9.99999990e-01 9.99473577e-01 9.98492771e-01 4.83073181e-02\n",
            "  9.99999998e-01 9.99995793e-01 1.65830363e-13 1.94542292e-15\n",
            "  2.92509075e-01 7.34638903e-02 5.25905072e-27 6.05059838e-09\n",
            "  3.77001200e-38 5.23256109e-34 2.76912732e-01 2.53708861e-01]]Loss[[0.2035706]]test loss:[[4.59570081]]\n",
            "iteration3890:[[9.99997042e-01 1.00000000e+00 1.00000000e+00 9.99999990e-01\n",
            "  9.99999990e-01 9.99487978e-01 9.98537825e-01 5.32232457e-02\n",
            "  9.99999998e-01 9.99995911e-01 1.69636314e-13 2.00112420e-15\n",
            "  2.91820036e-01 7.35645888e-02 5.38443459e-27 6.16764669e-09\n",
            "  3.85129245e-38 5.34084117e-34 2.75438110e-01 2.53092203e-01]]Loss[[0.19853561]]test loss:[[4.59105365]]\n",
            "iteration3900:[[9.99997134e-01 1.00000000e+00 1.00000000e+00 9.99999990e-01\n",
            "  9.99999990e-01 9.99501785e-01 9.98580922e-01 5.85597135e-02\n",
            "  9.99999998e-01 9.99996023e-01 1.73461376e-13 2.05756938e-15\n",
            "  2.91053909e-01 7.36416481e-02 5.51057846e-27 6.28441990e-09\n",
            "  3.93280131e-38 5.44920806e-34 2.73914316e-01 2.52410285e-01]]Loss[[0.19355459]]test loss:[[4.58650025]]\n",
            "iteration3910:[[9.99997221e-01 1.00000000e+00 1.00000000e+00 9.99999990e-01\n",
            "  9.99999990e-01 9.99514947e-01 9.98621939e-01 6.43336678e-02\n",
            "  9.99999998e-01 9.99996130e-01 1.77297709e-13 2.11466011e-15\n",
            "  2.90180375e-01 7.36854683e-02 5.63653235e-27 6.40061591e-09\n",
            "  4.01387123e-38 5.55679484e-34 2.72336029e-01 2.51635893e-01]]Loss[[0.18863052]]test loss:[[4.58203304]]\n",
            "iteration3920:[[9.99997305e-01 1.00000000e+00 1.00000000e+00 9.99999990e-01\n",
            "  9.99999990e-01 9.99527537e-01 9.98661094e-01 7.05649360e-02\n",
            "  9.99999998e-01 9.99996233e-01 1.81137223e-13 2.17229208e-15\n",
            "  2.89219377e-01 7.37020028e-02 5.76262439e-27 6.51592902e-09\n",
            "  4.09475177e-38 5.66388639e-34 2.70698744e-01 2.50787193e-01]]Loss[[0.18376961]]test loss:[[4.57767087]]\n",
            "iteration3930:[[9.99997383e-01 1.00000000e+00 1.00000000e+00 9.99999990e-01\n",
            "  9.99999990e-01 9.99539154e-01 9.98698298e-01 7.71903444e-02\n",
            "  9.99999998e-01 9.99996330e-01 1.84865155e-13 2.23038144e-15\n",
            "  2.87992777e-01 7.36830617e-02 5.88366926e-27 6.62569297e-09\n",
            "  4.17480902e-38 5.76585441e-34 2.68859501e-01 2.49840939e-01]]Loss[[0.17900382]]test loss:[[4.57344575]]\n",
            "iteration3940:[[9.99997457e-01 1.00000000e+00 1.00000000e+00 9.99999990e-01\n",
            "  9.99999990e-01 9.99550098e-01 9.98733828e-01 8.42566987e-02\n",
            "  9.99999998e-01 9.99996423e-01 1.88530971e-13 2.28892308e-15\n",
            "  2.86615185e-01 7.36385341e-02 6.00229801e-27 6.73201203e-09\n",
            "  4.25452881e-38 5.86508891e-34 2.66902098e-01 2.48825692e-01]]Loss[[0.17432145]]test loss:[[4.56931082]]\n",
            "iteration3950:[[9.99997527e-01 1.00000000e+00 1.00000000e+00 9.99999990e-01\n",
            "  9.99999990e-01 9.99560507e-01 9.98767570e-01 9.18039785e-02\n",
            "  9.99999998e-01 9.99996512e-01 1.92178432e-13 2.34778460e-15\n",
            "  2.85134256e-01 7.35589781e-02 6.11956348e-27 6.83672866e-09\n",
            "  4.33317433e-38 5.96252726e-34 2.64893217e-01 2.47714748e-01]]Loss[[0.16971115]]test loss:[[4.56528646]]\n",
            "iteration3960:[[9.99997593e-01 1.00000000e+00 1.00000000e+00 9.99999990e-01\n",
            "  9.99999990e-01 9.99570441e-01 9.98799705e-01 9.98343296e-02\n",
            "  9.99999998e-01 9.99996596e-01 1.95773794e-13 2.40652767e-15\n",
            "  2.83568252e-01 7.34499192e-02 6.23575179e-27 6.93852431e-09\n",
            "  4.41030571e-38 6.05759853e-34 2.62798939e-01 2.46524761e-01]]Loss[[0.16517964]]test loss:[[4.56136622]]\n",
            "iteration3970:[[9.99997657e-01 1.00000000e+00 1.00000000e+00 9.99999990e-01\n",
            "  9.99999990e-01 9.99579853e-01 9.98830124e-01 1.08354959e-01\n",
            "  9.99999998e-01 9.99996675e-01 1.99352810e-13 2.46557552e-15\n",
            "  2.81887761e-01 7.33019742e-02 6.34973703e-27 7.03890464e-09\n",
            "  4.48645279e-38 6.15094050e-34 2.60671637e-01 2.45229464e-01]]Loss[[0.16072754]]test loss:[[4.55756801]]\n",
            "iteration3980:[[9.99997717e-01 1.00000000e+00 1.00000000e+00 9.99999990e-01\n",
            "  9.99999990e-01 9.99588881e-01 9.98859003e-01 1.17371618e-01\n",
            "  9.99999998e-01 9.99996751e-01 2.02910168e-13 2.52448299e-15\n",
            "  2.80148088e-01 7.31209968e-02 6.46293523e-27 7.13770409e-09\n",
            "  4.56118950e-38 6.24298728e-34 2.58511552e-01 2.43846388e-01]]Loss[[0.15636085]]test loss:[[4.55389816]]\n",
            "iteration3990:[[9.99997774e-01 1.00000000e+00 1.00000000e+00 9.99999990e-01\n",
            "  9.99999990e-01 9.99597324e-01 9.98886249e-01 1.26852043e-01\n",
            "  9.99999998e-01 9.99996823e-01 2.06378113e-13 2.58307973e-15\n",
            "  2.78249562e-01 7.28982750e-02 6.57197212e-27 7.23226133e-09\n",
            "  4.63372723e-38 6.33077519e-34 2.56247750e-01 2.42351249e-01]]Loss[[0.15208035]]test loss:[[4.5503496]]\n",
            "iteration4000:[[9.99997828e-01 1.00000000e+00 1.00000000e+00 9.99999990e-01\n",
            "  9.99999990e-01 9.99605384e-01 9.98912193e-01 1.36809919e-01\n",
            "  9.99999998e-01 9.99996891e-01 2.09774794e-13 2.64152118e-15\n",
            "  2.76279189e-01 7.26401331e-02 6.67828832e-27 7.32436009e-09\n",
            "  4.70499964e-38 6.41642803e-34 2.53939357e-01 2.40762862e-01]]Loss[[0.14789019]]test loss:[[4.54694554]]\n",
            "iteration4010:[[9.99997879e-01 1.00000000e+00 1.00000000e+00 9.99999990e-01\n",
            "  9.99999990e-01 9.99612921e-01 9.98936488e-01 1.47197144e-01\n",
            "  9.99999998e-01 9.99996954e-01 2.13089632e-13 2.69901189e-15\n",
            "  2.74163359e-01 7.23421257e-02 6.78112517e-27 7.41170096e-09\n",
            "  4.77312186e-38 6.49760422e-34 2.51528932e-01 2.39069152e-01]]Loss[[0.14379485]]test loss:[[4.54366547]]\n",
            "iteration4020:[[9.99997927e-01 1.00000000e+00 1.00000000e+00 9.99999990e-01\n",
            "  9.99999990e-01 9.99619989e-01 9.98959293e-01 1.57998674e-01\n",
            "  9.99999998e-01 9.99997014e-01 2.16313360e-13 2.75570662e-15\n",
            "  2.71921749e-01 7.20007010e-02 6.87977113e-27 7.49497130e-09\n",
            "  4.83829311e-38 6.57454822e-34 2.49043974e-01 2.37260591e-01]]Loss[[0.13979566]]test loss:[[4.54051842]]\n",
            "iteration4030:[[9.99997973e-01 1.00000000e+00 1.00000000e+00 9.99999990e-01\n",
            "  9.99999990e-01 9.99626644e-01 9.98980770e-01 1.69185804e-01\n",
            "  9.99999998e-01 9.99997070e-01 2.19437053e-13 2.81144985e-15\n",
            "  2.69576545e-01 7.16226965e-02 6.97461379e-27 7.57389975e-09\n",
            "  4.90080364e-38 6.64761842e-34 2.46484710e-01 2.35357288e-01]]Loss[[0.13589782]]test loss:[[4.5374999]]\n",
            "iteration4040:[[9.99998016e-01 1.00000000e+00 1.00000000e+00 9.99999990e-01\n",
            "  9.99999990e-01 9.99632942e-01 9.99000911e-01 1.80738464e-01\n",
            "  9.99999998e-01 9.99997123e-01 2.22481393e-13 2.86609361e-15\n",
            "  2.67151078e-01 7.12050535e-02 7.06613002e-27 7.64938447e-09\n",
            "  4.96020407e-38 6.71725078e-34 2.43881866e-01 2.33351037e-01]]Loss[[0.13210214]]test loss:[[4.53460256]]\n",
            "iteration4050:[[9.99998056e-01 1.00000000e+00 1.00000000e+00 9.99999990e-01\n",
            "  9.99999990e-01 9.99638800e-01 9.99019854e-01 1.92613952e-01\n",
            "  9.99999998e-01 9.99997172e-01 2.25381388e-13 2.91982810e-15\n",
            "  2.64599981e-01 7.07454119e-02 7.15142352e-27 7.71991972e-09\n",
            "  5.01676178e-38 6.78180627e-34 2.41202525e-01 2.31235583e-01]]Loss[[0.12840592]]test loss:[[4.53185057]]\n",
            "iteration4060:[[9.99998094e-01 1.00000000e+00 1.00000000e+00 9.99999990e-01\n",
            "  9.99999990e-01 9.99644231e-01 9.99037478e-01 2.04769128e-01\n",
            "  9.99999998e-01 9.99997218e-01 2.28159353e-13 2.97187376e-15\n",
            "  2.61925356e-01 7.02511642e-02 7.23212254e-27 7.78454290e-09\n",
            "  5.06946716e-38 6.84105017e-34 2.38429512e-01 2.29032458e-01]]Loss[[0.12481142]]test loss:[[4.5292445]]\n",
            "iteration4070:[[9.99998129e-01 1.00000000e+00 1.00000000e+00 9.99999990e-01\n",
            "  9.99999990e-01 9.99649241e-01 9.99053928e-01 2.17155486e-01\n",
            "  9.99999998e-01 9.99997261e-01 2.30810787e-13 3.02245232e-15\n",
            "  2.59150562e-01 6.97207715e-02 7.30772234e-27 7.84413521e-09\n",
            "  5.11866233e-38 6.89542326e-34 2.35592731e-01 2.26737607e-01]]Loss[[0.12132317]]test loss:[[4.52678257]]\n",
            "iteration4080:[[9.99998162e-01 1.00000000e+00 1.00000000e+00 9.99999990e-01\n",
            "  9.99999990e-01 9.99653855e-01 9.99069255e-01 2.29747997e-01\n",
            "  9.99999998e-01 9.99997301e-01 2.33359748e-13 3.07146027e-15\n",
            "  2.56310658e-01 6.91557922e-02 7.37919960e-27 7.89971420e-09\n",
            "  5.16425988e-38 6.94580031e-34 2.32723251e-01 2.24355778e-01]]Loss[[0.11794092]]test loss:[[4.52445519]]\n",
            "iteration4090:[[9.99998193e-01 1.00000000e+00 1.00000000e+00 9.99999990e-01\n",
            "  9.99999990e-01 9.99658117e-01 9.99083633e-01 2.42506134e-01\n",
            "  9.99999998e-01 9.99997337e-01 2.35745908e-13 3.11914043e-15\n",
            "  2.53376742e-01 6.85581536e-02 7.44421860e-27 7.94996222e-09\n",
            "  5.20687890e-38 6.99103279e-34 2.29793872e-01 2.21892779e-01]]Loss[[0.11465978]]test loss:[[4.52227999]]\n",
            "iteration4100:[[9.99998222e-01 1.00000000e+00 1.00000000e+00 9.99999990e-01\n",
            "  9.99999990e-01 9.99661990e-01 9.99096833e-01 2.55360550e-01\n",
            "  9.99999998e-01 9.99997371e-01 2.37996389e-13 3.16477825e-15\n",
            "  2.50327905e-01 6.79273293e-02 7.50353307e-27 7.99414401e-09\n",
            "  5.24493027e-38 7.03018619e-34 2.26792808e-01 2.19347370e-01]]Loss[[0.1114811]]test loss:[[4.52023789]]\n",
            "iteration4110:[[9.99998250e-01 1.00000000e+00 1.00000000e+00 9.99999990e-01\n",
            "  9.99999990e-01 9.99665626e-01 9.99109118e-01 2.68297393e-01\n",
            "  9.99999998e-01 9.99997403e-01 2.40138348e-13 3.20864960e-15\n",
            "  2.47243126e-01 6.72714481e-02 7.55894377e-27 8.03431931e-09\n",
            "  5.27959158e-38 7.06565399e-34 2.23774245e-01 2.16742888e-01]]Loss[[0.10840748]]test loss:[[4.51832784]]\n",
            "iteration4120:[[9.99998275e-01 1.00000000e+00 1.00000000e+00 9.99999990e-01\n",
            "  9.99999990e-01 9.99668959e-01 9.99120604e-01 2.81277135e-01\n",
            "  9.99999998e-01 9.99997432e-01 2.42115863e-13 3.25104003e-15\n",
            "  2.44088432e-01 6.65900809e-02 7.60795340e-27 8.06932231e-09\n",
            "  5.31129897e-38 7.09613458e-34 2.20714651e-01 2.14078273e-01]]Loss[[0.10543239]]test loss:[[4.51655949]]\n",
            "iteration4130:[[9.99998299e-01 1.00000000e+00 1.00000000e+00 9.99999990e-01\n",
            "  9.99999990e-01 9.99671972e-01 9.99131125e-01 2.94235756e-01\n",
            "  9.99999998e-01 9.99997459e-01 2.43958147e-13 3.29128114e-15\n",
            "  2.40854135e-01 6.58857955e-02 7.65174423e-27 8.09854077e-09\n",
            "  5.33881955e-38 7.12111070e-34 2.17604230e-01 2.11360984e-01]]Loss[[0.10255673]]test loss:[[4.51491343]]\n",
            "iteration4140:[[9.99998321e-01 1.00000000e+00 1.00000000e+00 9.99999990e-01\n",
            "  9.99999990e-01 9.99674772e-01 9.99140842e-01 3.07161634e-01\n",
            "  9.99999998e-01 9.99997483e-01 2.45694578e-13 3.32969530e-15\n",
            "  2.37598242e-01 6.51613524e-02 7.69149770e-27 8.12406480e-09\n",
            "  5.36290242e-38 7.14241697e-34 2.14494260e-01 2.08598982e-01]]Loss[[0.09978056]]test loss:[[4.51338677]]\n",
            "iteration4150:[[9.99998342e-01 1.00000000e+00 1.00000000e+00 9.99999990e-01\n",
            "  9.99999990e-01 9.99677318e-01 9.99149876e-01 3.20026469e-01\n",
            "  9.99999998e-01 9.99997505e-01 2.47273484e-13 3.36660178e-15\n",
            "  2.34298804e-01 6.44193524e-02 7.72524235e-27 8.14485954e-09\n",
            "  5.38409843e-38 7.15898148e-34 2.11363786e-01 2.05799785e-01]]Loss[[0.0970975]]test loss:[[4.51197581]]\n",
            "iteration4160:[[9.99998361e-01 1.00000000e+00 1.00000000e+00 9.99999990e-01\n",
            "  9.99999990e-01 9.99679637e-01 9.99158128e-01 3.32774151e-01\n",
            "  9.99999998e-01 9.99997526e-01 2.48751901e-13 3.40138325e-15\n",
            "  2.30971724e-01 6.36624011e-02 7.75522958e-27 8.16140922e-09\n",
            "  5.40162163e-38 7.17163533e-34 2.08226032e-01 2.02970890e-01]]Loss[[0.09451041]]test loss:[[4.51068104]]\n",
            "iteration4170:[[9.99998379e-01 1.00000000e+00 1.00000000e+00 9.99999990e-01\n",
            "  9.99999990e-01 9.99681732e-01 9.99165812e-01 3.45405210e-01\n",
            "  9.99999998e-01 9.99997544e-01 2.50080930e-13 3.43469839e-15\n",
            "  2.27618470e-01 6.28931355e-02 7.77960579e-27 8.17363187e-09\n",
            "  5.41666844e-38 7.18014019e-34 2.05081208e-01 2.00119863e-01]]Loss[[0.09201186]]test loss:[[4.50950287]]\n",
            "iteration4180:[[9.99998396e-01 1.00000000e+00 1.00000000e+00 9.99999990e-01\n",
            "  9.99999990e-01 9.99683585e-01 9.99172782e-01 3.57864924e-01\n",
            "  9.99999998e-01 9.99997562e-01 2.51313268e-13 3.46626336e-15\n",
            "  2.24231476e-01 6.21140399e-02 7.79961215e-27 8.18187915e-09\n",
            "  5.42875800e-38 7.18482809e-34 2.01938524e-01 1.97253822e-01]]Loss[[0.08960311]]test loss:[[4.50841364]]\n",
            "iteration4190:[[9.99998411e-01 1.00000000e+00 1.00000000e+00 9.99999990e-01\n",
            "  9.99999990e-01 9.99685328e-01 9.99179263e-01 3.70191434e-01\n",
            "  9.99999998e-01 9.99997577e-01 2.52435537e-13 3.49612992e-15\n",
            "  2.20879617e-01 6.13274717e-02 7.81643768e-27 8.18733501e-09\n",
            "  5.43805801e-38 7.18675296e-34 1.98824093e-01 1.94379521e-01]]Loss[[0.08727852]]test loss:[[4.50743523]]\n",
            "iteration4200:[[9.99998426e-01 1.00000000e+00 1.00000000e+00 9.99999990e-01\n",
            "  9.99999990e-01 9.99686867e-01 9.99185148e-01 3.82307971e-01\n",
            "  9.99999998e-01 9.99997591e-01 2.53448530e-13 3.52404118e-15\n",
            "  2.17515525e-01 6.05377698e-02 7.82960215e-27 8.18845582e-09\n",
            "  5.44435675e-38 7.18485644e-34 1.95705884e-01 1.91509378e-01]]Loss[[0.08503832]]test loss:[[4.50653573]]\n",
            "iteration4210:[[9.99998440e-01 1.00000000e+00 1.00000000e+00 9.99999990e-01\n",
            "  9.99999990e-01 9.99688222e-01 9.99190583e-01 3.94222341e-01\n",
            "  9.99999998e-01 9.99997603e-01 2.54332810e-13 3.55065419e-15\n",
            "  2.14148600e-01 5.97427392e-02 7.83778996e-27 8.18614752e-09\n",
            "  5.44853050e-38 7.17944431e-34 1.92603655e-01 1.88637039e-01]]Loss[[0.08287677]]test loss:[[4.50573579]]\n",
            "iteration4220:[[9.99998452e-01 1.00000000e+00 1.00000000e+00 9.99999990e-01\n",
            "  9.99999990e-01 9.99689427e-01 9.99195468e-01 4.05909397e-01\n",
            "  9.99999998e-01 9.99997615e-01 2.55143432e-13 3.57543665e-15\n",
            "  2.10798681e-01 5.89466213e-02 7.84343876e-27 8.18091879e-09\n",
            "  5.44983858e-38 7.17124161e-34 1.89527962e-01 1.85774637e-01]]Loss[[0.08079453]]test loss:[[4.50500475]]\n",
            "iteration4230:[[9.99998464e-01 1.00000000e+00 1.00000000e+00 9.99999990e-01\n",
            "  9.99999990e-01 9.99690483e-01 9.99200001e-01 4.17376975e-01\n",
            "  9.99999998e-01 9.99997625e-01 2.55837022e-13 3.59903682e-15\n",
            "  2.07465555e-01 5.81511978e-02 7.84492301e-27 8.17272592e-09\n",
            "  5.44953726e-38 7.16023797e-34 1.86477708e-01 1.82927219e-01]]Loss[[0.07878591]]test loss:[[4.50435418]]\n",
            "iteration4240:[[9.99998475e-01 1.00000000e+00 1.00000000e+00 9.99999990e-01\n",
            "  9.99999990e-01 9.99691410e-01 9.99204050e-01 4.28599785e-01\n",
            "  9.99999998e-01 9.99997634e-01 2.56466433e-13 3.62093076e-15\n",
            "  2.04160097e-01 5.73580423e-02 7.84427694e-27 8.16203509e-09\n",
            "  5.44669200e-38 7.14686250e-34 1.83461766e-01 1.80099235e-01]]Loss[[0.076851]]test loss:[[4.50377634]]\n",
            "iteration4250:[[9.99998485e-01 1.00000000e+00 1.00000000e+00 9.99999990e-01\n",
            "  9.99999990e-01 9.99692220e-01 9.99207832e-01 4.39591129e-01\n",
            "  9.99999998e-01 9.99997641e-01 2.56990373e-13 3.64176429e-15\n",
            "  2.00887737e-01 5.65705875e-02 7.84022836e-27 8.14882153e-09\n",
            "  5.44271694e-38 7.13134666e-34 1.80478972e-01 1.77300484e-01]]Loss[[0.07498507]]test loss:[[4.50326471]]\n",
            "iteration4260:[[9.99998495e-01 1.00000000e+00 1.00000000e+00 9.99999990e-01\n",
            "  9.99999990e-01 9.99692939e-01 9.99211236e-01 4.50354076e-01\n",
            "  9.99999998e-01 9.99997649e-01 2.57478591e-13 3.66157885e-15\n",
            "  1.97661203e-01 5.57864200e-02 7.83417192e-27 8.13489366e-09\n",
            "  5.43732206e-38 7.11481819e-34 1.77563677e-01 1.74523885e-01]]Loss[[0.07318638]]test loss:[[4.50281694]]\n",
            "iteration4270:[[9.99998503e-01 1.00000000e+00 1.00000000e+00 9.99999990e-01\n",
            "  9.99999990e-01 9.99693521e-01 9.99214245e-01 4.60849568e-01\n",
            "  9.99999998e-01 9.99997655e-01 2.57874930e-13 3.67961651e-15\n",
            "  1.94457116e-01 5.50086655e-02 7.82571678e-27 8.11755109e-09\n",
            "  5.42934881e-38 7.09519809e-34 1.74662000e-01 1.71778344e-01]]Loss[[0.07145176]]test loss:[[4.50241978]]\n",
            "iteration4280:[[9.99998512e-01 1.00000000e+00 1.00000000e+00 9.99999990e-01\n",
            "  9.99999990e-01 9.99693998e-01 9.99217029e-01 4.71095859e-01\n",
            "  9.99999998e-01 9.99997660e-01 2.58181545e-13 3.69676714e-15\n",
            "  1.91291841e-01 5.42383591e-02 7.81423941e-27 8.09826248e-09\n",
            "  5.42043412e-38 7.07377778e-34 1.71801579e-01 1.69066705e-01]]Loss[[0.06977885]]test loss:[[4.502088]]\n",
            "iteration4290:[[9.99998519e-01 1.00000000e+00 1.00000000e+00 9.99999990e-01\n",
            "  9.99999990e-01 9.99694400e-01 9.99219491e-01 4.81099021e-01\n",
            "  9.99999998e-01 9.99997664e-01 2.58446763e-13 3.71254559e-15\n",
            "  1.88178243e-01 5.34779887e-02 7.80182356e-27 8.07742949e-09\n",
            "  5.40989711e-38 7.05113078e-34 1.68988718e-01 1.66396026e-01]]Loss[[0.06816584]]test loss:[[4.50180413]]\n",
            "iteration4300:[[9.99998527e-01 1.00000000e+00 1.00000000e+00 9.99999990e-01\n",
            "  9.99999990e-01 9.99694734e-01 9.99221740e-01 4.90867758e-01\n",
            "  9.99999998e-01 9.99997668e-01 2.58652982e-13 3.72754862e-15\n",
            "  1.85119300e-01 5.27251736e-02 7.78730546e-27 8.05573693e-09\n",
            "  5.39846198e-38 7.02744578e-34 1.66234691e-01 1.63759267e-01]]Loss[[0.06660949]]test loss:[[4.50156022]]\n",
            "iteration4310:[[9.99998533e-01 1.00000000e+00 1.00000000e+00 9.99999990e-01\n",
            "  9.99999990e-01 9.99694957e-01 9.99223752e-01 5.00370463e-01\n",
            "  9.99999998e-01 9.99997671e-01 2.58783908e-13 3.74156552e-15\n",
            "  1.82093267e-01 5.19822851e-02 7.77034955e-27 8.03194806e-09\n",
            "  5.38593987e-38 7.00188827e-34 1.63514884e-01 1.61163102e-01]]Loss[[0.06510833]]test loss:[[4.50136661]]\n",
            "iteration4320:[[9.99998540e-01 1.00000000e+00 1.00000000e+00 9.99999990e-01\n",
            "  9.99999990e-01 9.99695122e-01 9.99225501e-01 5.09641835e-01\n",
            "  9.99999998e-01 9.99997674e-01 2.58901731e-13 3.75464359e-15\n",
            "  1.79124479e-01 5.12512694e-02 7.75288574e-27 8.00766430e-09\n",
            "  5.37258757e-38 6.97604583e-34 1.60857296e-01 1.58613014e-01]]Loss[[0.06366019]]test loss:[[4.50121465]]\n",
            "iteration4330:[[9.99998545e-01 1.00000000e+00 1.00000000e+00 9.99999990e-01\n",
            "  9.99999990e-01 9.99695237e-01 9.99227089e-01 5.18683011e-01\n",
            "  9.99999998e-01 9.99997676e-01 2.58955246e-13 3.76684639e-15\n",
            "  1.76214868e-01 5.05297528e-02 7.73380652e-27 7.98229852e-09\n",
            "  5.35818757e-38 6.94903522e-34 1.58249344e-01 1.56101935e-01]]Loss[[0.06226179]]test loss:[[4.50109347]]\n",
            "iteration4340:[[9.99998551e-01 1.00000000e+00 1.00000000e+00 9.99999990e-01\n",
            "  9.99999990e-01 9.99695258e-01 9.99228487e-01 5.27470096e-01\n",
            "  9.99999998e-01 9.99997677e-01 2.58945690e-13 3.77821917e-15\n",
            "  1.73343651e-01 4.98195977e-02 7.71276802e-27 7.95527904e-09\n",
            "  5.34299109e-38 6.92057743e-34 1.55679140e-01 1.53635069e-01]]Loss[[0.06091202]]test loss:[[4.50101681]]\n",
            "iteration4350:[[9.99998556e-01 1.00000000e+00 1.00000000e+00 9.99999990e-01\n",
            "  9.99999990e-01 9.99695237e-01 9.99229670e-01 5.36037763e-01\n",
            "  9.99999998e-01 9.99997678e-01 2.58915441e-13 3.78856337e-15\n",
            "  1.70532224e-01 4.91223729e-02 7.69156515e-27 7.92752413e-09\n",
            "  5.32681445e-38 6.89165541e-34 1.53161594e-01 1.51216770e-01]]Loss[[0.05960838]]test loss:[[4.5009701]]\n",
            "iteration4360:[[9.99998561e-01 1.00000000e+00 1.00000000e+00 9.99999990e-01\n",
            "  9.99999990e-01 9.99695179e-01 9.99230732e-01 5.44385346e-01\n",
            "  9.99999998e-01 9.99997679e-01 2.58848519e-13 3.79842646e-15\n",
            "  1.67782076e-01 4.84358598e-02 7.66915799e-27 7.89961681e-09\n",
            "  5.31028587e-38 6.86241526e-34 1.50705249e-01 1.48840384e-01]]Loss[[0.05834949]]test loss:[[4.5009432]]\n",
            "iteration4370:[[9.99998565e-01 1.00000000e+00 1.00000000e+00 9.99999990e-01\n",
            "  9.99999990e-01 9.99695051e-01 9.99231664e-01 5.52495325e-01\n",
            "  9.99999998e-01 9.99997679e-01 2.58728418e-13 3.80758807e-15\n",
            "  1.65076843e-01 4.77625557e-02 7.64538409e-27 7.87040094e-09\n",
            "  5.29333198e-38 6.83223096e-34 1.48287890e-01 1.46512973e-01]]Loss[[0.05713379]]test loss:[[4.5009568]]\n"
          ]
        }
      ],
      "source": [
        "from numpy import random\n",
        "HOG_nn = nn(data_input, 250, test_input)\n",
        "HOG_nn.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[7.96258897e-13 5.07085538e-06 1.09504089e-07 1.74108319e-01\n  7.73218200e-13 4.33168518e-34 1.47117814e-01 3.44989732e-15\n  9.84174832e-01 2.82480584e-09]]\n"
          ]
        }
      ],
      "source": [
        "result = HOG_nn.pred(test_input.T)\n",
        "#print probability result\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "matrix([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "result[result >= 0.5] = 1\n",
        "result[result < 0.5] = 0\n",
        "result"
      ]
    },
    {
      "source": [
        "## **Train a neural network with layer size of 500 neurons** "
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration0:[[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]]Loss0test loss:[[7.13903477]]\n",
            "iteration10:[[9.98572569e-01 9.99999990e-01 1.00000000e+00 1.00000000e+00\n",
            "  9.75612079e-01 1.52942996e-05 3.18569277e-10 9.99999990e-01\n",
            "  1.00000000e+00 9.99999990e-01 1.09643916e-24 1.00000000e+00\n",
            "  9.99999990e-01 1.00000000e+00 9.99999990e-01 1.46428766e-04\n",
            "  1.02348810e-07 6.26479734e-18 9.99999990e-01 9.99999990e-01]]Loss[[8.38896581]]test loss:[[7.13244355]]\n",
            "iteration20:[[9.98676733e-01 9.99999990e-01 1.00000000e+00 1.00000000e+00\n",
            "  9.78426853e-01 2.03986939e-05 4.33199906e-10 9.99999990e-01\n",
            "  1.00000000e+00 9.99999990e-01 1.14953688e-24 1.00000000e+00\n",
            "  9.99999990e-01 1.00000000e+00 9.99999990e-01 1.55731458e-04\n",
            "  1.10868322e-07 7.06440362e-18 9.99999990e-01 9.99999990e-01]]Loss[[8.3402005]]test loss:[[7.12594144]]\n",
            "iteration30:[[9.98772230e-01 9.99999990e-01 1.00000000e+00 1.00000000e+00\n",
            "  9.80894120e-01 2.71812326e-05 5.88524827e-10 9.99999990e-01\n",
            "  1.00000000e+00 9.99999990e-01 1.20433866e-24 1.00000000e+00\n",
            "  9.99999990e-01 1.00000000e+00 9.99999990e-01 1.65489168e-04\n",
            "  1.19992887e-07 7.95925726e-18 9.99999990e-01 9.99999990e-01]]Loss[[8.29354976]]test loss:[[7.11951758]]\n",
            "iteration40:[[9.98859964e-01 9.99999990e-01 1.00000000e+00 1.00000000e+00\n",
            "  9.83061436e-01 3.61890956e-05 7.98880520e-10 9.99999990e-01\n",
            "  1.00000000e+00 9.99999990e-01 1.26095312e-24 1.00000000e+00\n",
            "  9.99999990e-01 1.00000000e+00 9.99999990e-01 1.75731215e-04\n",
            "  1.29769333e-07 8.96071612e-18 9.99999990e-01 9.99999990e-01]]Loss[[8.24837093]]test loss:[[7.11316263]]\n",
            "iteration50:[[9.98940714e-01 9.99999990e-01 1.00000000e+00 1.00000000e+00\n",
            "  9.84968898e-01 4.81469911e-05 1.08363106e-09 9.99999990e-01\n",
            "  1.00000000e+00 9.99999990e-01 1.31948961e-24 1.00000000e+00\n",
            "  9.99999990e-01 1.00000000e+00 9.99999990e-01 1.86488170e-04\n",
            "  1.40247832e-07 1.00814841e-17 9.99999990e-01 9.99999990e-01]]Loss[[8.20427581]]test loss:[[7.10686856]]\n",
            "iteration60:[[9.99015156e-01 9.99999990e-01 1.00000000e+00 1.00000000e+00\n",
            "  9.86650505e-01 6.40145422e-05 1.46892592e-09 9.99999990e-01\n",
            "  1.00000000e+00 9.99999990e-01 1.38005849e-24 1.00000000e+00\n",
            "  9.99999990e-01 1.00000000e+00 9.99999990e-01 1.97791939e-04\n",
            "  1.51482145e-07 1.13357693e-17 9.99999990e-01 9.99999990e-01]]Loss[[8.15574965]]test loss:[[7.10062838]]\n",
            "iteration70:[[9.99083880e-01 9.99999990e-01 1.00000000e+00 1.00000000e+00\n",
            "  9.88135227e-01 8.50621534e-05 1.99007099e-09 9.99999990e-01\n",
            "  9.99999990e-01 9.99999990e-01 1.44277131e-24 1.00000000e+00\n",
            "  9.99999990e-01 1.00000000e+00 9.99999990e-01 2.09675828e-04\n",
            "  1.63529866e-07 1.27394595e-17 9.99999990e-01 9.99999990e-01]]Loss[[8.10530764]]test loss:[[7.09443607]]\n",
            "iteration80:[[9.99147397e-01 9.99999990e-01 1.00000000e+00 1.00000000e+00\n",
            "  9.89447864e-01 1.12971267e-04 2.69472590e-09 9.99999990e-01\n",
            "  9.99999990e-01 9.99999990e-01 1.50774100e-24 1.00000000e+00\n",
            "  9.99999990e-01 1.00000000e+00 9.99999990e-01 2.22174599e-04\n",
            "  1.76452678e-07 1.43103166e-17 9.99999990e-01 9.99999990e-01]]Loss[[8.05763891]]test loss:[[7.08828643]]\n",
            "iteration90:[[9.99206173e-01 9.99999990e-01 1.00000000e+00 1.00000000e+00\n",
            "  9.90609734e-01 1.49966787e-04 3.64721511e-09 9.99999990e-01\n",
            "  9.99999990e-01 9.99999990e-01 1.57508178e-24 1.00000000e+00\n",
            "  9.99999990e-01 1.00000000e+00 9.99999990e-01 2.35324507e-04\n",
            "  1.90316607e-07 1.60681928e-17 9.99999990e-01 9.99999990e-01]]Loss[[8.01179525]]test loss:[[7.08217496]]\n",
            "iteration100:[[9.99260615e-01 9.99999990e-01 1.00000000e+00 1.00000000e+00\n",
            "  9.91642136e-01 1.99067689e-04 4.93905914e-09 9.99999990e-01\n",
            "  9.99999990e-01 9.99999990e-01 1.64534244e-24 1.00000000e+00\n",
            "  9.99999990e-01 1.00000000e+00 9.99999990e-01 2.49239923e-04\n",
            "  2.05192187e-07 1.80352598e-17 9.99999990e-01 9.99999990e-01]]Loss[[7.96282792]]test loss:[[7.07606454]]\n",
            "iteration110:[[9.99311087e-01 9.99999990e-01 1.00000000e+00 1.00000000e+00\n",
            "  9.92557741e-01 2.64150678e-04 6.68660570e-09 9.99999990e-01\n",
            "  9.99999990e-01 9.99999990e-01 1.71829348e-24 1.00000000e+00\n",
            "  9.99999990e-01 1.00000000e+00 9.99999990e-01 2.63901392e-04\n",
            "  2.21154706e-07 2.02362691e-17 9.99999990e-01 9.99999990e-01]]Loss[[7.91610133]]test loss:[[7.06998525]]\n",
            "iteration120:[[9.99357911e-01 9.99999990e-01 1.00000000e+00 1.00000000e+00\n",
            "  9.93370142e-01 3.50380499e-04 9.04937919e-09 9.99999990e-01\n",
            "  9.99999990e-01 9.99999990e-01 1.79400789e-24 1.00000000e+00\n",
            "  9.99999990e-01 1.00000000e+00 9.99999990e-01 2.79342353e-04\n",
            "  2.38284604e-07 2.26988516e-17 9.99999990e-01 9.99999990e-01]]Loss[[7.86805292]]test loss:[[7.06393435]]\n",
            "iteration130:[[9.99401379e-01 9.99999990e-01 1.00000000e+00 1.00000000e+00\n",
            "  9.94091487e-01 4.64592063e-04 1.22431996e-08 9.99999990e-01\n",
            "  9.99999990e-01 9.99999990e-01 1.87260428e-24 1.00000000e+00\n",
            "  9.99999990e-01 1.00000000e+00 9.99999990e-01 2.95605608e-04\n",
            "  2.56667476e-07 2.54538105e-17 9.99999990e-01 9.99999990e-01]]Loss[[7.81968414]]test loss:[[7.05790965]]\n",
            "iteration140:[[9.99441753e-01 9.99999990e-01 1.00000000e+00 1.00000000e+00\n",
            "  9.94732380e-01 6.15813748e-04 1.65593334e-08 9.99999990e-01\n",
            "  9.99999990e-01 9.99999990e-01 1.95419944e-24 1.00000000e+00\n",
            "  9.99999990e-01 1.00000000e+00 9.99999990e-01 3.12735034e-04\n",
            "  2.76394116e-07 2.85354421e-17 9.99999990e-01 9.99999990e-01]]Loss[[7.77246283]]test loss:[[7.05192629]]\n",
            "iteration150:[[9.99479268e-01 9.99999990e-01 1.00000000e+00 1.00000000e+00\n",
            "  9.95302097e-01 8.15961753e-04 2.23907347e-08 9.99999990e-01\n",
            "  9.99999990e-01 9.99999990e-01 2.03890624e-24 1.00000000e+00\n",
            "  9.99999990e-01 1.00000000e+00 9.99999990e-01 3.31029536e-04\n",
            "  2.97560427e-07 3.19818688e-17 9.99999990e-01 9.99999990e-01]]Loss[[7.72469106]]test loss:[[7.04597063]]\n",
            "iteration160:[[9.99514140e-01 9.99999990e-01 1.00000000e+00 1.00000000e+00\n",
            "  9.95808765e-01 1.08074740e-03 3.02673218e-08 9.99999990e-01\n",
            "  9.99999990e-01 9.99999990e-01 2.12683074e-24 1.00000000e+00\n",
            "  9.99999990e-01 1.00000000e+00 9.99999990e-01 3.50598487e-04\n",
            "  3.20267096e-07 3.58353680e-17 9.99999990e-01 9.99999990e-01]]Loss[[7.67698746]]test loss:[[7.04003791]]\n",
            "iteration170:[[9.99546560e-01 9.99999990e-01 1.00000000e+00 1.00000000e+00\n",
            "  9.96259522e-01 1.43085813e-03 4.09033460e-08 9.99999990e-01\n",
            "  9.99999990e-01 9.99999990e-01 2.21806844e-24 1.00000000e+00\n",
            "  9.99999990e-01 1.00000000e+00 9.99999990e-01 3.71234299e-04\n",
            "  3.44619040e-07 4.01426938e-17 9.99999990e-01 9.99999990e-01]]Loss[[7.62968024]]test loss:[[7.03412835]]\n",
            "iteration180:[[9.99576703e-01 9.99999990e-01 1.00000000e+00 1.00000000e+00\n",
            "  9.96660641e-01 1.89348548e-03 5.52608668e-08 9.99999990e-01\n",
            "  9.99999990e-01 9.99999990e-01 2.31269869e-24 1.00000000e+00\n",
            "  9.99999990e-01 1.00000000e+00 9.99999990e-01 3.93059733e-04\n",
            "  3.70724357e-07 4.49553441e-17 9.99999990e-01 9.99999990e-01]]Loss[[7.58173256]]test loss:[[7.02824299]]\n",
            "iteration190:[[9.99604729e-01 9.99999990e-01 1.00000000e+00 1.00000000e+00\n",
            "  9.97017646e-01 2.50428749e-03 7.46346147e-08 9.99999990e-01\n",
            "  9.99999990e-01 9.99999990e-01 2.41077705e-24 1.00000000e+00\n",
            "  9.99999990e-01 1.00000000e+00 9.99999990e-01 4.16049500e-04\n",
            "  3.98692657e-07 5.03297380e-17 9.99999990e-01 9.99999990e-01]]Loss[[7.5341549]]test loss:[[7.02238386]]\n",
            "iteration200:[[9.99630778e-01 9.99999990e-01 1.00000000e+00 1.00000000e+00\n",
            "  9.97334296e-01 3.30988306e-03 1.00773710e-07 9.99999990e-01\n",
            "  9.99999990e-01 9.99999990e-01 2.51232512e-24 1.00000000e+00\n",
            "  9.99999990e-01 1.00000000e+00 9.99999990e-01 4.40080746e-04\n",
            "  4.28632562e-07 5.63272443e-17 9.99999990e-01 9.99999990e-01]]Loss[[7.48660127]]test loss:[[7.01657636]]\n",
            "iteration210:[[9.99654980e-01 9.99999990e-01 1.00000000e+00 1.00000000e+00\n",
            "  9.97615565e-01 4.37097829e-03 1.36018126e-07 9.99999990e-01\n",
            "  9.99999990e-01 9.99999990e-01 2.61731701e-24 1.00000000e+00\n",
            "  9.99999990e-01 1.00000000e+00 9.99999990e-01 4.65211102e-04\n",
            "  4.60648068e-07 6.30139707e-17 9.99999990e-01 9.99999990e-01]]Loss[[7.43901032]]test loss:[[7.01081253]]\n",
            "iteration220:[[9.99677450e-01 9.99999990e-01 1.00000000e+00 1.00000000e+00\n",
            "  9.97865997e-01 5.76620115e-03 1.83499583e-07 9.99999990e-01\n",
            "  9.99999990e-01 9.99999990e-01 2.72565864e-24 1.00000000e+00\n",
            "  9.99999990e-01 1.00000000e+00 9.99999990e-01 4.91553385e-04\n",
            "  4.94832720e-07 7.04600966e-17 9.99999990e-01 9.99999990e-01]]Loss[[7.39145285]]test loss:[[7.00508956]]\n",
            "iteration230:[[9.99698289e-01 9.99999990e-01 1.00000000e+00 1.00000000e+00\n",
            "  9.98088872e-01 7.59667972e-03 2.47408831e-07 9.99999990e-01\n",
            "  9.99999990e-01 9.99999990e-01 2.83716347e-24 1.00000000e+00\n",
            "  9.99999990e-01 1.00000000e+00 9.99999990e-01 5.19097607e-04\n",
            "  5.31262031e-07 7.87386874e-17 9.99999990e-01 9.99999990e-01]]Loss[[7.34403389]]test loss:[[6.99941661]]\n",
            "iteration240:[[9.99717381e-01 9.99999990e-01 1.00000000e+00 1.00000000e+00\n",
            "  9.98287087e-01 9.99127107e-03 3.33329963e-07 9.99999990e-01\n",
            "  9.99999990e-01 9.99999990e-01 2.95151991e-24 1.00000000e+00\n",
            "  9.99999990e-01 1.00000000e+00 9.99999990e-01 5.47803535e-04\n",
            "  5.69982766e-07 8.79236357e-17 9.99999990e-01 9.99999990e-01]]Loss[[7.29653657]]test loss:[[6.99380588]]\n",
            "iteration250:[[9.99735034e-01 9.99999990e-01 1.00000000e+00 1.00000000e+00\n",
            "  9.98463192e-01 1.31121440e-02 4.48672930e-07 9.99999990e-01\n",
            "  9.99999990e-01 9.99999990e-01 3.06824936e-24 1.00000000e+00\n",
            "  9.99999990e-01 1.00000000e+00 9.99999990e-01 5.77589049e-04\n",
            "  6.10998344e-07 9.80864637e-17 9.99999990e-01 9.99999990e-01]]Loss[[7.24916083]]test loss:[[6.98827347]]\n",
            "iteration260:[[9.99751311e-01 9.99999990e-01 1.00000000e+00 1.00000000e+00\n",
            "  9.98619436e-01 1.71600313e-02 6.03218047e-07 9.99999990e-01\n",
            "  9.99999990e-01 9.99999990e-01 3.18608928e-24 1.00000000e+00\n",
            "  9.99999990e-01 1.00000000e+00 9.99999990e-01 6.08315307e-04\n",
            "  6.54249512e-07 1.09291441e-16 9.99999990e-01 9.99999990e-01]]Loss[[7.2018678]]test loss:[[6.98284022]]\n",
            "iteration270:[[9.99766263e-01 9.99999990e-01 1.00000000e+00 1.00000000e+00\n",
            "  9.98757798e-01 2.23779372e-02 8.09785806e-07 9.99999990e-01\n",
            "  9.99999990e-01 9.99999990e-01 3.30352460e-24 1.00000000e+00\n",
            "  9.99999990e-01 1.00000000e+00 9.99999990e-01 6.39768602e-04\n",
            "  6.99589689e-07 1.21588681e-16 9.99999990e-01 9.99999990e-01]]Loss[[7.15475055]]test loss:[[6.97753284]]\n",
            "iteration280:[[9.99779925e-01 9.99999990e-01 1.00000000e+00 1.00000000e+00\n",
            "  9.98880016e-01 2.90513145e-02 1.08503533e-06 9.99999990e-01\n",
            "  9.99999990e-01 9.99999990e-01 3.42022011e-24 1.00000000e+00\n",
            "  9.99999990e-01 1.00000000e+00 9.99999990e-01 6.71639258e-04\n",
            "  7.46754817e-07 1.35004723e-16 9.99999990e-01 9.99999990e-01]]Loss[[7.10777066]]test loss:[[6.97238492]]\n",
            "iteration290:[[9.99792326e-01 9.99999990e-01 1.00000000e+00 1.00000000e+00\n",
            "  9.98987619e-01 3.75018797e-02 1.45037610e-06 9.99999990e-01\n",
            "  9.99999990e-01 9.99999990e-01 3.53443045e-24 1.00000000e+00\n",
            "  9.99999990e-01 1.00000000e+00 9.99999990e-01 7.03499037e-04\n",
            "  7.95328819e-07 1.49530385e-16 9.99999990e-01 9.99999990e-01]]Loss[[7.06097019]]test loss:[[6.96743799]]\n",
            "iteration300:[[9.99803479e-01 9.99999990e-01 1.00000000e+00 1.00000000e+00\n",
            "  9.99081949e-01 4.80716038e-02 1.93294808e-06 9.99999990e-01\n",
            "  9.99999990e-01 9.99999990e-01 3.64393134e-24 1.00000000e+00\n",
            "  9.99999990e-01 1.00000000e+00 9.99999990e-01 7.34780194e-04\n",
            "  8.44707793e-07 1.65106137e-16 9.99999990e-01 9.99999990e-01]]Loss[[7.01442393]]test loss:[[6.96274224]]\n",
            "iteration310:[[9.99813389e-01 9.99999990e-01 1.00000000e+00 1.00000000e+00\n",
            "  9.99164190e-01 6.10936127e-02 2.56658596e-06 9.99999990e-01\n",
            "  9.99999990e-01 9.99999990e-01 3.74598866e-24 1.00000000e+00\n",
            "  9.99999990e-01 1.00000000e+00 9.99999990e-01 7.64761439e-04\n",
            "  8.94068978e-07 1.81606118e-16 9.99999990e-01 9.99999990e-01]]Loss[[6.96819979]]test loss:[[6.95835662]]\n",
            "iteration320:[[9.99822050e-01 9.99999990e-01 1.00000000e+00 1.00000000e+00\n",
            "  9.99235391e-01 7.68486277e-02 3.39264222e-06 9.99999990e-01\n",
            "  9.99999990e-01 9.99999990e-01 3.83738206e-24 1.00000000e+00\n",
            "  9.99999990e-01 1.00000000e+00 9.99999990e-01 7.92568344e-04\n",
            "  9.42353949e-07 1.98823160e-16 9.99999990e-01 9.99999990e-01]]Loss[[6.92234103]]test loss:[[6.95434807]]\n",
            "iteration330:[[9.99829452e-01 9.99999990e-01 1.00000000e+00 1.00000000e+00\n",
            "  9.99296490e-01 9.55098913e-02 4.46051692e-06 9.99999990e-01\n",
            "  9.99999990e-01 9.99999990e-01 3.91451080e-24 1.00000000e+00\n",
            "  9.99999990e-01 9.99999999e-01 9.99999990e-01 8.17196881e-04\n",
            "  9.88278180e-07 2.16458607e-16 9.99999990e-01 9.99999990e-01]]Loss[[6.87688291]]test loss:[[6.95078934]]\n",
            "iteration340:[[9.99835580e-01 9.99999990e-01 1.00000000e+00 1.00000000e+00\n",
            "  9.99348338e-01 1.17085931e-01 5.82775621e-06 9.99999990e-01\n",
            "  9.99999990e-01 9.99999990e-01 3.97360238e-24 1.00000000e+00\n",
            "  9.99999990e-01 9.99999999e-01 9.99999990e-01 8.37567324e-04\n",
            "  1.03037888e-06 2.34121697e-16 9.99999990e-01 9.99999990e-01]]Loss[[6.8318962]]test loss:[[6.94775543]]\n",
            "iteration350:[[9.99840419e-01 9.99999990e-01 1.00000000e+00 1.00000000e+00\n",
            "  9.99391715e-01 1.41376700e-01 7.55965627e-06 9.99999990e-01\n",
            "  9.99999990e-01 9.99999990e-01 4.01102336e-24 1.00000000e+00\n",
            "  9.99999990e-01 9.99999999e-01 9.99999990e-01 8.52610147e-04\n",
            "  1.06710772e-06 2.51342936e-16 9.99999990e-01 9.99999990e-01]]Loss[[6.78741731]]test loss:[[6.94531866]]\n",
            "iteration360:[[9.99843954e-01 9.99999990e-01 1.00000000e+00 1.00000000e+00\n",
            "  9.99427348e-01 1.67960441e-01 9.72844667e-06 9.99999990e-01\n",
            "  9.99999990e-01 9.99999990e-01 4.02365970e-24 1.00000000e+00\n",
            "  9.99999990e-01 9.99999999e-01 9.99999990e-01 8.61375940e-04\n",
            "  1.09696374e-06 2.67603534e-16 9.99999990e-01 9.99999990e-01]]Loss[[6.74346845]]test loss:[[6.94354304]]\n",
            "iteration370:[[9.99846176e-01 9.99999990e-01 1.00000000e+00 1.00000000e+00\n",
            "  9.99455915e-01 1.96222411e-01 1.24122852e-05 9.99999990e-01\n",
            "  9.99999990e-01 9.99999990e-01 4.00929949e-24 1.00000000e+00\n",
            "  9.99999990e-01 9.99999999e-01 9.99999990e-01 8.63150682e-04\n",
            "  1.11864732e-06 2.82378574e-16 9.99999990e-01 9.99999990e-01]]Loss[[6.700067]]test loss:[[6.94247886]]\n",
            "iteration380:[[9.99847080e-01 9.99999990e-01 1.00000000e+00 1.00000000e+00\n",
            "  9.99478050e-01 2.25423073e-01 1.56944073e-05 9.99999990e-01\n",
            "  9.99999990e-01 9.99999990e-01 3.96693373e-24 1.00000000e+00\n",
            "  9.99999990e-01 9.99999998e-01 9.99999990e-01 8.57551138e-04\n",
            "  1.13120480e-06 2.95186841e-16 9.99999990e-01 9.99999990e-01]]Loss[[6.65719241]]test loss:[[6.94215862]]\n",
            "iteration390:[[9.99846662e-01 9.99999990e-01 1.00000000e+00 1.00000000e+00\n",
            "  9.99494338e-01 2.54788798e-01 1.96627741e-05 9.99999990e-01\n",
            "  9.99999990e-01 9.99999990e-01 3.89690313e-24 1.00000000e+00\n",
            "  9.99999990e-01 9.99999998e-01 9.99999990e-01 8.44577390e-04\n",
            "  1.13413146e-06 3.05637204e-16 9.99999990e-01 9.99999990e-01]]Loss[[6.61481114]]test loss:[[6.94259512]]\n",
            "iteration400:[[9.99844966e-01 9.99999990e-01 1.00000000e+00 1.00000000e+00\n",
            "  9.99505307e-01 2.83663057e-01 2.44103844e-05 9.99999990e-01\n",
            "  9.99999990e-01 9.99999990e-01 3.80169922e-24 1.00000000e+00\n",
            "  9.99999990e-01 9.99999997e-01 9.99999990e-01 8.24608658e-04\n",
            "  1.12769839e-06 3.13543709e-16 9.99999990e-01 9.99999990e-01]]Loss[[6.57289521]]test loss:[[6.94375861]]\n"
          ]
        }
      ],
      "source": [
        "# random.seed(1)\n",
        "HOG_nn_500 = nn(data_input, 500, test_input)\n",
        "HOG_nn_500.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "probability result:\n[[9.99999997e-01 3.12256353e-14 1.00000000e+00 1.50151626e-23\n  1.00000000e+00 1.10860741e-06 1.00000000e+00 4.46865026e-36\n  1.00000000e+00 1.00000000e+00]]\n[[1. 0. 1. 0. 1. 0. 1. 0. 1. 1.]]\n"
          ]
        }
      ],
      "source": [
        "result = HOG_nn_500.pred(test_input.T)\n",
        "print(\"probability result:\")\n",
        "print(result)\n",
        "result[result >= 0.5] = 1\n",
        "result[result < 0.5] = 0\n",
        "print(result)"
      ]
    },
    {
      "source": [
        "## **Train a neural network with layer size of 1000 neurons**"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "tags": [
          "outputPrepend"
        ]
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".99999990e-01 9.99999990e-01 9.99999990e-01 9.99999990e-01\n",
            "  1.00000000e+00 4.70148491e-01 1.13187613e-21 9.99999990e-01]]Loss[[12.51871589]]test loss:[[9.17138265]]\n",
            "iteration50:[[4.99229746e-48 9.99999990e-01 9.99999990e-01 9.99999990e-01\n",
            "  9.99999990e-01 9.81480667e-01 9.99999990e-01 9.99999990e-01\n",
            "  9.99999990e-01 9.99999990e-01 1.91821681e-10 9.99999990e-01\n",
            "  9.99999990e-01 9.99999990e-01 9.99999990e-01 9.99999990e-01\n",
            "  1.00000000e+00 3.59460034e-01 8.80499799e-22 9.99999990e-01]]Loss[[12.45580547]]test loss:[[9.14334148]]\n",
            "iteration60:[[7.37377820e-48 9.99999990e-01 9.99999990e-01 9.99999990e-01\n",
            "  9.99999990e-01 9.77360126e-01 9.99999990e-01 9.99999990e-01\n",
            "  9.99999990e-01 9.99999990e-01 1.58397122e-10 9.99999990e-01\n",
            "  9.99999990e-01 9.99999990e-01 9.99999990e-01 9.99999990e-01\n",
            "  1.00000000e+00 2.82436438e-01 7.23017433e-22 9.99999990e-01]]Loss[[12.39685807]]test loss:[[9.11950893]]\n",
            "iteration70:[[1.07833687e-47 9.99999990e-01 9.99999990e-01 9.99999990e-01\n",
            "  9.99999990e-01 9.72056900e-01 9.99999990e-01 9.99999990e-01\n",
            "  9.99999990e-01 9.99999990e-01 1.28496229e-10 9.99999990e-01\n",
            "  9.99999990e-01 1.00000000e+00 9.99999990e-01 9.99999990e-01\n",
            "  1.00000000e+00 2.20306233e-01 5.81453170e-22 9.99999990e-01]]Loss[[13.22034279]]test loss:[[9.0921886]]\n",
            "iteration80:[[1.07125136e-47 9.99999990e-01 9.99999990e-01 9.99999990e-01\n",
            "  9.99999990e-01 9.47484303e-01 9.99999990e-01 9.99999990e-01\n",
            "  9.99999990e-01 9.99999990e-01 7.04463931e-11 9.99999990e-01\n",
            "  9.99999990e-01 1.00000000e+00 9.99999990e-01 9.99999990e-01\n",
            "  1.00000000e+00 1.25652398e-01 2.90243447e-22 9.99999990e-01]]Loss[[13.12409705]]test loss:[[9.04598996]]\n",
            "iteration90:[[1.12923625e-47 9.99999990e-01 9.99999990e-01 9.99999990e-01\n",
            "  9.99999990e-01 9.09985621e-01 9.99999990e-01 9.99999990e-01\n",
            "  9.99999990e-01 9.99999990e-01 4.04783769e-11 9.99999990e-01\n",
            "  9.99999990e-01 1.00000000e+00 9.99999990e-01 9.99999990e-01\n",
            "  1.00000000e+00 7.45002898e-02 1.53443415e-22 9.99999990e-01]]Loss[[13.01132954]]test loss:[[9.00862934]]\n",
            "iteration100:[[1.25042741e-47 9.99999990e-01 9.99999990e-01 9.99999990e-01\n",
            "  9.99999990e-01 8.58724075e-01 9.99999990e-01 9.99999990e-01\n",
            "  9.99999990e-01 9.99999990e-01 2.41943477e-11 9.99999990e-01\n",
            "  9.99999990e-01 1.00000000e+00 9.99999990e-01 9.99999990e-01\n",
            "  1.00000000e+00 4.61244945e-02 8.50609192e-23 9.99999990e-01]]Loss[[12.90373193]]test loss:[[8.97836778]]\n",
            "iteration110:[[1.45166486e-47 9.99999990e-01 9.99999990e-01 1.00000000e+00\n",
            "  9.99999990e-01 7.97150056e-01 9.99999990e-01 9.99999990e-01\n",
            "  9.99999990e-01 9.99999990e-01 1.50245981e-11 9.99999990e-01\n",
            "  9.99999990e-01 1.00000000e+00 9.99999990e-01 9.99999990e-01\n",
            "  1.00000000e+00 2.99010628e-02 4.93413927e-23 9.99999990e-01]]Loss[[12.79894133]]test loss:[[8.95317263]]\n",
            "iteration120:[[1.76559054e-47 9.99999990e-01 9.99999990e-01 1.00000000e+00\n",
            "  9.99999990e-01 7.32359982e-01 9.99999990e-01 9.99999990e-01\n",
            "  9.99999990e-01 9.99999990e-01 9.68986176e-12 9.99999990e-01\n",
            "  9.99999990e-01 1.00000000e+00 9.99999990e-01 9.99999990e-01\n",
            "  1.00000000e+00 2.03154739e-02 2.99253988e-23 9.99999990e-01]]Loss[[12.69665048]]test loss:[[8.93122723]]\n",
            "iteration130:[[2.24454683e-47 9.99999990e-01 9.99999990e-01 1.00000000e+00\n",
            "  9.99999990e-01 6.71812764e-01 9.99999990e-01 9.99999990e-01\n",
            "  9.99999990e-01 9.99999990e-01 6.47878971e-12 9.99999990e-01\n",
            "  9.99999990e-01 1.00000000e+00 9.99999990e-01 9.99999990e-01\n",
            "  1.00000000e+00 1.44413611e-02 1.89385327e-23 9.99999990e-01]]Loss[[12.59652409]]test loss:[[8.91124564]]\n",
            "iteration140:[[2.96351252e-47 9.99999990e-01 9.99999990e-01 1.00000000e+00\n",
            "  9.99999990e-01 6.19750135e-01 9.99999990e-01 9.99999990e-01\n",
            "  9.99999990e-01 9.99999990e-01 4.46767989e-12 9.99999990e-01\n",
            "  9.99999990e-01 1.00000000e+00 9.99999990e-01 9.99999990e-01\n",
            "  9.99999999e-01 1.06733360e-02 1.24297217e-23 9.99999990e-01]]Loss[[12.49802899]]test loss:[[9.77355876]]\n",
            "iteration150:[[4.03725088e-47 9.99999990e-01 9.99999990e-01 1.00000000e+00\n",
            "  9.99999990e-01 5.77411996e-01 9.99999990e-01 9.99999990e-01\n",
            "  9.99999990e-01 9.99999990e-01 3.16064816e-12 9.99999990e-01\n",
            "  9.99999990e-01 1.00000000e+00 9.99999990e-01 9.99999990e-01\n",
            "  9.99999998e-01 8.14766646e-03 8.40567310e-24 9.99999990e-01]]Loss[[12.40063672]]test loss:[[9.75542066]]\n",
            "iteration160:[[5.63746145e-47 9.99999990e-01 9.99999990e-01 1.00000000e+00\n",
            "  9.99999990e-01 5.44055945e-01 9.99999990e-01 9.99999990e-01\n",
            "  9.99999990e-01 9.99999990e-01 2.28152809e-12 9.99999990e-01\n",
            "  9.99999990e-01 1.00000000e+00 9.99999990e-01 9.99999990e-01\n",
            "  9.99999996e-01 6.38059958e-03 5.81989029e-24 9.99999990e-01]]Loss[[12.30387194]]test loss:[[9.73771418]]\n",
            "iteration170:[[8.02502636e-47 9.99999990e-01 9.99999990e-01 1.00000000e+00\n",
            "  9.99999990e-01 5.18267032e-01 9.99999990e-01 9.99999990e-01\n",
            "  9.99999990e-01 9.99999990e-01 1.67306115e-12 9.99999990e-01\n",
            "  9.99999990e-01 1.00000000e+00 9.99999990e-01 9.99999990e-01\n",
            "  9.99999991e-01 5.09762589e-03 4.10437606e-24 9.99999990e-01]]Loss[[12.20749238]]test loss:[[9.68563047]]\n",
            "iteration180:[[1.15940152e-46 9.99999990e-01 9.99999990e-01 1.00000000e+00\n",
            "  9.99999990e-01 4.98509822e-01 9.99999990e-01 9.99999990e-01\n",
            "  9.99999990e-01 9.99999990e-01 1.24179572e-12 9.99999990e-01\n",
            "  9.99999990e-01 1.00000000e+00 9.99999990e-01 9.99999990e-01\n",
            "  9.99999980e-01 4.13575343e-03 2.93573938e-24 9.99999990e-01]]Loss[[12.1113517]]test loss:[[9.64811519]]\n",
            "iteration190:[[1.69388625e-46 9.99999990e-01 9.99999990e-01 1.00000000e+00\n",
            "  9.99999990e-01 4.83405998e-01 9.99999990e-01 9.99999990e-01\n",
            "  9.99999990e-01 9.99999990e-01 9.30175413e-13 9.99999990e-01\n",
            "  9.99999990e-01 1.00000000e+00 9.99999990e-01 9.99999990e-01\n",
            "  9.99999958e-01 3.39477140e-03 2.12241216e-24 9.99999990e-01]]Loss[[12.01535507]]test loss:[[9.61661987]]\n",
            "iteration200:[[2.49646060e-46 9.99999990e-01 9.99999990e-01 1.00000000e+00\n",
            "  9.99999990e-01 4.71926663e-01 9.99999990e-01 9.99999990e-01\n",
            "  9.99999990e-01 9.99999990e-01 7.01748546e-13 9.99999990e-01\n",
            "  9.99999990e-01 1.00000000e+00 9.99999990e-01 9.99999990e-01\n",
            "  9.99999913e-01 2.81207617e-03 1.54729142e-24 9.99999990e-01]]Loss[[11.91943978]]test loss:[[9.5884418]]\n",
            "iteration210:[[3.70355224e-46 9.99999990e-01 9.99999990e-01 1.00000000e+00\n",
            "  9.99999990e-01 4.63186854e-01 9.99999990e-01 9.99999990e-01\n",
            "  9.99999990e-01 9.99999990e-01 5.32272504e-13 9.99999990e-01\n",
            "  9.99999990e-01 1.00000000e+00 9.99999990e-01 9.99999990e-01\n",
            "  9.99999819e-01 2.34549764e-03 1.13512788e-24 9.99999990e-01]]Loss[[11.82358025]]test loss:[[9.55465982]]\n",
            "iteration220:[[5.52132954e-46 9.99999990e-01 9.99999990e-01 1.00000000e+00\n",
            "  9.99999990e-01 4.56502809e-01 9.99999990e-01 9.99999990e-01\n",
            "  9.99999990e-01 9.99999990e-01 4.05351522e-13 9.99999990e-01\n",
            "  9.99999990e-01 1.00000000e+00 9.99999990e-01 9.99999990e-01\n",
            "  9.99999626e-01 1.96646538e-03 8.36667615e-25 9.99999990e-01]]Loss[[11.72775271]]test loss:[[9.52518585]]\n",
            "iteration230:[[8.26220673e-46 9.99999990e-01 9.99999990e-01 1.00000000e+00\n",
            "  9.99999990e-01 4.51492338e-01 9.99999990e-01 9.99999990e-01\n",
            "  9.99999990e-01 9.99999990e-01 3.09662597e-13 9.99999990e-01\n",
            "  9.99999990e-01 1.00000000e+00 9.99999990e-01 9.99999990e-01\n",
            "  9.99999231e-01 1.65586228e-03 6.19140720e-25 9.99999990e-01]]Loss[[11.63197369]]test loss:[[9.49392795]]\n",
            "iteration240:[[1.24007569e-45 9.99999990e-01 9.99999990e-01 1.00000000e+00\n",
            "  9.99999990e-01 4.47802409e-01 9.99999990e-01 9.99999990e-01\n",
            "  9.99999990e-01 9.99999990e-01 2.37133396e-13 9.99999990e-01\n",
            "  9.99999990e-01 9.99999999e-01 9.99999990e-01 9.99999990e-01\n",
            "  9.99998421e-01 1.39925230e-03 4.59664357e-25 9.99999990e-01]]Loss[[11.53623036]]test loss:[[9.46270019]]\n",
            "iteration250:[[1.86502816e-45 9.99999990e-01 9.99999990e-01 1.00000000e+00\n",
            "  9.99999990e-01 4.44965641e-01 9.99999990e-01 9.99999990e-01\n",
            "  9.99999990e-01 9.99999990e-01 1.81895628e-13 9.99999990e-01\n",
            "  9.99999990e-01 9.99999998e-01 9.99999990e-01 9.99999990e-01\n",
            "  9.99996768e-01 1.18499059e-03 3.41929800e-25 9.99999990e-01]]Loss[[11.44050089]]test loss:[[9.43471775]]\n",
            "iteration260:[[2.80983806e-45 9.99999990e-01 9.99999990e-01 1.00000000e+00\n",
            "  9.99999990e-01 4.42835758e-01 9.99999990e-01 9.99999990e-01\n",
            "  9.99999990e-01 9.99999990e-01 1.39725717e-13 9.99999990e-01\n",
            "  9.99999990e-01 9.99999996e-01 9.99999990e-01 9.99999990e-01\n",
            "  9.99993395e-01 1.00542215e-03 2.54782468e-25 9.99999990e-01]]Loss[[11.34477826]]test loss:[[9.40289257]]\n",
            "iteration270:[[4.23813035e-45 9.99999990e-01 9.99999990e-01 1.00000000e+00\n",
            "  9.99999990e-01 4.41190086e-01 9.99999990e-01 9.99999990e-01\n",
            "  9.99999990e-01 9.99999990e-01 1.07433505e-13 9.99999990e-01\n",
            "  9.99999990e-01 9.99999992e-01 9.99999990e-01 9.99999990e-01\n",
            "  9.99986519e-01 8.54130471e-04 1.90054101e-25 9.99999990e-01]]Loss[[11.2490611]]test loss:[[9.37308861]]\n",
            "iteration280:[[6.39855128e-45 9.99999990e-01 9.99999990e-01 1.00000000e+00\n",
            "  9.99999990e-01 4.39937472e-01 9.99999990e-01 9.99999990e-01\n",
            "  9.99999990e-01 9.99999990e-01 8.26696317e-14 9.99999990e-01\n",
            "  9.99999990e-01 9.99999983e-01 9.99999990e-01 9.99999990e-01\n",
            "  9.99972514e-01 7.26365011e-04 1.42074424e-25 9.99999990e-01]]Loss[[11.15335212]]test loss:[[9.34260578]]\n",
            "iteration290:[[9.66841047e-45 9.99999990e-01 9.99999990e-01 1.00000000e+00\n",
            "  9.99999990e-01 4.39019257e-01 9.99999990e-01 9.99999990e-01\n",
            "  9.99999990e-01 9.99999990e-01 6.36585690e-14 9.99999990e-01\n",
            "  9.99999990e-01 9.99999962e-01 9.99999990e-01 9.99999990e-01\n",
            "  9.99944007e-01 6.18285484e-04 1.06348663e-25 9.99999990e-01]]Loss[[11.05764782]]test loss:[[9.31168816]]\n",
            "iteration300:[[1.46290680e-44 9.99999990e-01 9.99999990e-01 1.00000000e+00\n",
            "  9.99999990e-01 4.38555255e-01 9.99999990e-01 9.99999990e-01\n",
            "  9.99999990e-01 9.99999990e-01 4.90750192e-14 9.99999990e-01\n",
            "  9.99999990e-01 9.99999916e-01 9.99999990e-01 9.99999990e-01\n",
            "  9.99886191e-01 5.27054802e-04 7.97196694e-26 9.99999990e-01]]Loss[[10.96200338]]test loss:[[9.28136089]]\n",
            "iteration310:[[2.21281176e-44 9.99999990e-01 9.99999990e-01 1.00000000e+00\n",
            "  9.99999990e-01 4.38445679e-01 9.99999990e-01 9.99999990e-01\n",
            "  9.99999990e-01 9.99999990e-01 3.78680343e-14 9.99999990e-01\n",
            "  9.99999990e-01 9.99999816e-01 9.99999990e-01 9.99999990e-01\n",
            "  9.99769255e-01 4.49436096e-04 5.98119080e-26 9.99999990e-01]]Loss[[10.86651598]]test loss:[[9.25148871]]\n",
            "iteration320:[[3.34361314e-44 9.99999990e-01 9.99999990e-01 1.00000000e+00\n",
            "  9.99999990e-01 4.38553112e-01 9.99999990e-01 9.99999990e-01\n",
            "  9.99999990e-01 9.99999990e-01 2.92389353e-14 9.99999990e-01\n",
            "  9.99999990e-01 9.99999595e-01 9.99999990e-01 9.99999990e-01\n",
            "  9.99532829e-01 3.83340458e-04 4.48810426e-26 9.99999990e-01]]Loss[[10.7711663]]test loss:[[9.22131645]]\n",
            "iteration330:[[5.05323307e-44 9.99999990e-01 9.99999990e-01 1.00000000e+00\n",
            "  9.99999990e-01 4.38709496e-01 9.99999990e-01 9.99999990e-01\n",
            "  9.99999990e-01 9.99999990e-01 2.25800638e-14 9.99999990e-01\n",
            "  9.99999990e-01 9.99999109e-01 9.99999990e-01 9.99999990e-01\n",
            "  9.99054637e-01 3.27043369e-04 3.36848281e-26 9.99999990e-01]]Loss[[10.67583824]]test loss:[[9.19110502]]\n",
            "iteration340:[[7.63799824e-44 9.99999990e-01 9.99999990e-01 1.00000000e+00\n",
            "  9.99999990e-01 4.38890401e-01 9.99999990e-01 9.99999990e-01\n",
            "  9.99999990e-01 1.00000000e+00 1.74399178e-14 9.99999990e-01\n",
            "  9.99999990e-01 9.99998041e-01 9.99999990e-01 9.99999990e-01\n",
            "  9.98089095e-01 2.79062119e-04 2.52856497e-26 9.99999990e-01]]Loss[[10.58056469]]test loss:[[9.16110297]]\n",
            "iteration350:[[1.15532239e-43 9.99999990e-01 9.99999990e-01 1.00000000e+00\n",
            "  9.99999990e-01 4.39252859e-01 9.99999990e-01 9.99999990e-01\n",
            "  9.99999990e-01 1.00000000e+00 1.34783623e-14 9.99999990e-01\n",
            "  9.99999990e-01 9.99995696e-01 9.99999990e-01 9.99999990e-01\n",
            "  9.96146707e-01 2.38304291e-04 1.89958280e-26 9.99999990e-01]]Loss[[10.48539279]]test loss:[[9.13103747]]\n",
            "iteration360:[[1.74984509e-43 9.99999990e-01 9.99999990e-01 1.00000000e+00\n",
            "  9.99999990e-01 4.39940763e-01 9.99999990e-01 9.99999990e-01\n",
            "  9.99999990e-01 1.00000000e+00 1.04286117e-14 9.99999990e-01\n",
            "  9.99999990e-01 9.99990554e-01 9.99999990e-01 9.99999990e-01\n",
            "  9.92265100e-01 2.03778284e-04 1.42910307e-26 9.99999990e-01]]Loss[[10.39041233]]test loss:[[9.10125506]]\n",
            "iteration370:[[2.65659993e-43 9.99999990e-01 9.99999990e-01 1.00000000e+00\n",
            "  9.99999990e-01 4.41199628e-01 9.99999990e-01 9.99999990e-01\n",
            "  9.99999990e-01 1.00000000e+00 8.08551434e-15 9.99999990e-01\n",
            "  9.99999990e-01 9.99979318e-01 9.99999990e-01 9.99999990e-01\n",
            "  9.84612100e-01 1.74677183e-04 1.07790003e-26 9.99999990e-01]]Loss[[10.29581757]]test loss:[[9.0716121]]\n",
            "iteration380:[[4.05321412e-43 9.99999990e-01 9.99999990e-01 1.00000000e+00\n",
            "  9.99999990e-01 4.43661731e-01 9.99999990e-01 9.99999990e-01\n",
            "  9.99999990e-01 1.00000000e+00 6.29562373e-15 9.99999990e-01\n",
            "  9.99999990e-01 9.99954942e-01 9.99999990e-01 9.99999990e-01\n",
            "  9.69919143e-01 1.50484909e-04 8.17334536e-27 9.99999990e-01]]Loss[[10.20196565]]test loss:[[9.0424549]]\n",
            "iteration390:[[6.24211074e-43 9.99999990e-01 9.99999990e-01 1.00000000e+00\n",
            "  9.99999990e-01 4.48397204e-01 9.99999990e-01 9.99999990e-01\n",
            "  9.99999990e-01 1.00000000e+00 4.94155842e-15 9.99999990e-01\n",
            "  9.99999990e-01 9.99902753e-01 9.99999990e-01 9.99999990e-01\n",
            "  9.43055700e-01 1.30874697e-04 6.26007618e-27 9.99999990e-01]]Loss[[10.10946967]]test loss:[[9.01411025]]\n",
            "iteration400:[[9.76734493e-43 9.99999990e-01 9.99999990e-01 1.00000000e+00\n",
            "  9.99999990e-01 4.56987590e-01 9.99999990e-01 9.99999990e-01\n",
            "  9.99999990e-01 1.00000000e+00 3.93288724e-15 9.99999990e-01\n",
            "  9.99999990e-01 9.99793471e-01 9.99999990e-01 9.99999990e-01\n",
            "  8.97882972e-01 1.15673267e-04 4.87784772e-27 9.99999990e-01]]Loss[[10.01927184]]test loss:[[8.98712105]]\n",
            "iteration410:[[1.56661203e-42 9.99999990e-01 9.99999990e-01 1.00000000e+00\n",
            "  9.99999990e-01 4.71423100e-01 9.99999990e-01 9.99999990e-01\n",
            "  9.99999990e-01 1.00000000e+00 3.19704978e-15 9.99999990e-01\n",
            "  9.99999990e-01 9.99572148e-01 9.99999990e-01 9.99999990e-01\n",
            "  8.31002333e-01 1.04808185e-04 3.90305486e-27 9.99999990e-01]]Loss[[9.93248845]]test loss:[[8.96219326]]\n",
            "iteration420:[[2.59540749e-42 9.99999990e-01 9.99999990e-01 1.00000000e+00\n",
            "  9.99999990e-01 4.93487650e-01 9.99999990e-01 9.99999990e-01\n",
            "  9.99999990e-01 1.00000000e+00 2.67449584e-15 9.99999990e-01\n",
            "  9.99999990e-01 9.99143597e-01 9.99999990e-01 9.99999990e-01\n",
            "  7.46853889e-01 9.81724853e-05 3.23708619e-27 9.99999990e-01]]Loss[[9.85005738]]test loss:[[8.9397835]]\n",
            "iteration430:[[4.45331976e-42 9.99999990e-01 9.99999990e-01 1.00000000e+00\n",
            "  9.99999990e-01 5.23053566e-01 9.99999990e-01 9.99999990e-01\n",
            "  9.99999990e-01 1.00000000e+00 2.30675050e-15 9.99999990e-01\n",
            "  9.99999990e-01 9.98346755e-01 9.99999990e-01 9.99999990e-01\n",
            "  6.56921158e-01 9.52666604e-05 2.79015704e-27 9.99999990e-01]]Loss[[9.77206524]]test loss:[[8.92015]]\n",
            "iteration440:[[7.88169364e-42 9.99999990e-01 9.99999990e-01 1.00000000e+00\n",
            "  9.99999990e-01 5.58137795e-01 9.99999990e-01 9.99999990e-01\n",
            "  9.99999990e-01 1.00000000e+00 2.04462970e-15 9.99999990e-01\n",
            "  9.99999990e-01 9.96912242e-01 9.99999990e-01 9.99999990e-01\n",
            "  5.72452813e-01 9.53713479e-05 2.48962371e-27 9.99999990e-01]]Loss[[9.69803401]]test loss:[[8.90302425]]\n",
            "iteration450:[[1.42909110e-41 9.99999990e-01 9.99999990e-01 1.00000000e+00\n",
            "  9.99999990e-01 5.96013131e-01 9.99999990e-01 9.99999990e-01\n",
            "  9.99999990e-01 1.00000000e+00 1.85156365e-15 9.99999990e-01\n",
            "  9.99999990e-01 9.94389051e-01 9.99999990e-01 9.99999990e-01\n",
            "  4.99385212e-01 9.78151842e-05 2.28397771e-27 9.99999990e-01]]Loss[[9.6272449]]test loss:[[8.88791236]]\n",
            "iteration460:[[2.63473465e-41 9.99999990e-01 9.99999990e-01 1.00000000e+00\n",
            "  9.99999990e-01 6.33958494e-01 9.99999990e-01 9.99999990e-01\n",
            "  9.99999990e-01 1.00000000e+00 1.70261573e-15 9.99999990e-01\n",
            "  9.99999990e-01 9.90022514e-01 9.99999990e-01 9.99999990e-01\n",
            "  4.38576161e-01 1.01998070e-04 2.13772851e-27 9.99999990e-01]]Loss[[9.55905936]]test loss:[[8.8743272]]\n",
            "iteration470:[[4.91132177e-41 9.99999990e-01 9.99999990e-01 1.00000000e+00\n",
            "  9.99999990e-01 6.70217459e-01 9.99999990e-01 9.99999990e-01\n",
            "  9.99999990e-01 1.00000000e+00 1.58286046e-15 9.99999990e-01\n",
            "  9.99999990e-01 9.82592602e-01 9.99999990e-01 9.99999990e-01\n",
            "  3.88769495e-01 1.07563160e-04 2.03036333e-27 9.99999990e-01]]Loss[[9.49306256]]test loss:[[8.86188524]]\n",
            "iteration480:[[9.23388267e-41 9.99999990e-01 9.99999990e-01 1.00000000e+00\n",
            "  9.99999990e-01 7.04051646e-01 9.99999990e-01 9.99999990e-01\n",
            "  9.99999990e-01 1.00000000e+00 1.48472799e-15 9.99999990e-01\n",
            "  9.99999990e-01 9.70246693e-01 9.99999990e-01 9.99999990e-01\n",
            "  3.48308134e-01 1.14416870e-04 1.95185760e-27 9.99999990e-01]]Loss[[9.42902647]]test loss:[[8.85045805]]\n",
            "iteration490:[[1.75059036e-40 9.99999990e-01 9.99999990e-01 1.00000000e+00\n",
            "  9.99999990e-01 7.35391986e-01 9.99999990e-01 9.99999990e-01\n",
            "  9.99999990e-01 1.00000000e+00 1.40511404e-15 9.99999990e-01\n",
            "  9.99999990e-01 9.50452821e-01 9.99999990e-01 9.99999990e-01\n",
            "  3.15770115e-01 1.22698673e-04 1.89874505e-27 9.99999990e-01]]Loss[[9.366931]]test loss:[[8.83996943]]\n",
            "iteration500:[[3.35209269e-40 9.99999990e-01 9.99999990e-01 1.00000000e+00\n",
            "  9.99999990e-01 7.64663074e-01 9.99999990e-01 9.99999990e-01\n",
            "  9.99999990e-01 1.00000000e+00 1.34396608e-15 9.99999990e-01\n",
            "  9.99999990e-01 9.20399340e-01 9.99999990e-01 9.99999990e-01\n",
            "  2.90221921e-01 1.32864559e-04 1.87319482e-27 9.99999990e-01]]Loss[[9.30696056]]test loss:[[8.83055201]]\n",
            "iteration510:[[6.50172532e-40 9.99999990e-01 9.99999990e-01 1.00000000e+00\n",
            "  9.99999990e-01 7.92417990e-01 9.99999990e-01 9.99999990e-01\n",
            "  9.99999990e-01 1.00000000e+00 1.30341414e-15 9.99999990e-01\n",
            "  9.99999990e-01 8.78136715e-01 9.99999990e-01 9.99999990e-01\n",
            "  2.71092832e-01 1.45699455e-04 1.88120733e-27 9.99999990e-01]]Loss[[9.24943081]]test loss:[[8.82239252]]\n",
            "iteration520:[[1.28103904e-39 9.99999990e-01 9.99999990e-01 1.00000000e+00\n",
            "  9.99999990e-01 8.18983288e-01 9.99999990e-01 9.99999990e-01\n",
            "  9.99999990e-01 1.00000000e+00 1.28602078e-15 9.99999990e-01\n",
            "  9.99999990e-01 8.24174667e-01 9.99999990e-01 9.99999990e-01\n",
            "  2.57893148e-01 1.62265962e-04 1.93062903e-27 9.99999990e-01]]Loss[[9.19464655]]test loss:[[8.81573384]]\n",
            "iteration530:[[2.57136218e-39 9.99999990e-01 9.99999990e-01 1.00000000e+00\n",
            "  9.99999990e-01 8.44472744e-01 9.99999990e-01 9.99999990e-01\n",
            "  9.99999990e-01 1.00000000e+00 1.29453972e-15 9.99999990e-01\n",
            "  9.99999990e-01 7.62430415e-01 9.99999990e-01 9.99999990e-01\n",
            "  2.50273043e-01 1.84049790e-04 2.03211956e-27 9.99999990e-01]]Loss[[9.14271772]]test loss:[[8.81071442]]\n",
            "iteration540:[[5.25811113e-39 9.99999990e-01 9.99999990e-01 1.00000000e+00\n",
            "  9.99999990e-01 8.68442225e-01 9.99999990e-01 9.99999990e-01\n",
            "  9.99999990e-01 1.00000000e+00 1.33009927e-15 9.99999990e-01\n",
            "  9.99999990e-01 6.98710689e-01 9.99999990e-01 9.99999990e-01\n",
            "  2.47502425e-01 2.12588121e-04 2.19427255e-27 9.99999990e-01]]Loss[[9.09348796]]test loss:[[8.80735791]]\n",
            "iteration550:[[1.09348459e-38 9.99999990e-01 9.99999990e-01 1.00000000e+00\n",
            "  9.99999990e-01 8.90354897e-01 9.99999990e-01 9.99999990e-01\n",
            "  9.99999990e-01 1.00000000e+00 1.39274896e-15 9.99999990e-01\n",
            "  9.99999990e-01 6.38354960e-01 9.99999990e-01 9.99999990e-01\n",
            "  2.48741637e-01 2.49646611e-04 2.42612318e-27 9.99999990e-01]]Loss[[9.04658752]]test loss:[[8.80554617]]\n",
            "iteration560:[[2.30555500e-38 9.99999990e-01 9.99999990e-01 1.00000000e+00\n",
            "  9.99999990e-01 9.09739446e-01 9.99999990e-01 9.99999990e-01\n",
            "  9.99999990e-01 1.00000000e+00 1.48180803e-15 9.99999990e-01\n",
            "  9.99999990e-01 5.84512242e-01 9.99999990e-01 9.99999990e-01\n",
            "  2.53040471e-01 2.97117360e-04 2.73629332e-27 9.99999990e-01]]Loss[[9.00156282]]test loss:[[8.80503815]]\n",
            "iteration570:[[4.91218122e-38 9.99999990e-01 9.99999990e-01 1.00000000e+00\n",
            "  9.99999990e-01 9.26397085e-01 9.99999990e-01 9.99999990e-01\n",
            "  9.99999990e-01 1.00000000e+00 1.59629539e-15 9.99999990e-01\n",
            "  9.99999990e-01 5.38209546e-01 9.99999990e-01 9.99999990e-01\n",
            "  2.59513324e-01 3.57193208e-04 3.13490189e-27 9.99999990e-01]]Loss[[8.95799102]]test loss:[[8.80554347]]\n",
            "iteration580:[[1.05435445e-37 9.99999990e-01 9.99999990e-01 1.00000000e+00\n",
            "  9.99999990e-01 9.40391207e-01 9.99999990e-01 9.99999990e-01\n",
            "  9.99999990e-01 1.00000000e+00 1.73604282e-15 9.99999990e-01\n",
            "  9.99999990e-01 4.99154104e-01 9.99999990e-01 9.99999990e-01\n",
            "  2.67424678e-01 4.32523398e-04 3.63476497e-27 9.99999990e-01]]Loss[[8.91553307]]test loss:[[8.80681923]]\n"
          ]
        }
      ],
      "source": [
        "HOG_nn_1000 = nn(data_input, 1000, test_input)\n",
        "HOG_nn_1000.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "probability result:\n[[2.47667760e-09 9.99499761e-01 1.00000000e+00 7.38954563e-41\n  9.89073402e-01 9.64289540e-01 6.64197595e-06 1.00000000e+00\n  9.99997632e-01 1.00000000e+00]]\n[[0. 1. 1. 0. 1. 1. 0. 1. 1. 1.]]\n"
          ]
        }
      ],
      "source": [
        "result = HOG_nn_1000.pred(test_input.T)\n",
        "print(\"probability result:\")\n",
        "print(result)\n",
        "result[result >= 0.5] = 1\n",
        "result[result < 0.5] = 0\n",
        "print(result)"
      ]
    }
  ]
}